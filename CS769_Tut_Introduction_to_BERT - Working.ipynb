{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "vy8f3I-0HqGW",
      "metadata": {
        "id": "vy8f3I-0HqGW"
      },
      "source": [
        "# Introduction to BERT\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this tutorial, we'll explore the basics of BERT (Bidirectional Encoder Representations from Transformers), learn how to tokenize text, use BERT for masked language modeling, fine-tune it on a classification task (both using Trainer and a manual loop), and run simple inference. At the end, there is a quiz and a discussion section\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Objectives\n",
        "- Understand BERT's architecture and use cases\n",
        "- Load and run a pretrained BERT model with Hugging Face's Transformers\n",
        "- Tokenize input text for BERT\n",
        "- Perform masked language modeling\n",
        "- Fine-tune BERT on a downstream classification task using Trainer\n",
        "- Fine-tune BERT manually with a custom training loop\n",
        "- Run inference with the fine-tuned model\n",
        "- Understand how contextual embeddings differ from previous approaches\n",
        "\n",
        "## 2. Prerequisites\n",
        "- Python 3.7+\n",
        "- `pip install transformers torch datasets`\n",
        "- Basic familiarity with Python and Jupyter notebooks"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "xMm_opk-H023",
      "metadata": {
        "id": "xMm_opk-H023"
      },
      "source": [
        "## 3. Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "93e86b2b",
      "metadata": {},
      "outputs": [],
      "source": [
        "# !pip install transformers torch datasets\n",
        "# !pip install 'accelerate>=0.26.0'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "959c9993",
      "metadata": {
        "id": "959c9993"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using MPS device.\n"
          ]
        }
      ],
      "source": [
        "# Set fixed random seed for reproducibility\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "# Device configuration\n",
        "if torch.backends.mps.is_available():\n",
        "    # Check if MPS is available and enabled\n",
        "    if torch.backends.mps.is_built():\n",
        "        device = torch.device(\"mps\")\n",
        "        print(\"Using MPS device.\")\n",
        "    else:\n",
        "        device = torch.device(\"cpu\")\n",
        "        print(\"MPS is available but not built. Falling back to CPU.\")\n",
        "else:\n",
        "    device = torch.device(\"cpu\")\n",
        "    print(\"MPS is not available. Falling back to CPU.\")\n",
        "\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory allocated: {torch.cuda.memory_allocated(0)/1024**3:.1f} GB\")\n",
        "    print(f\"Memory cached: {torch.cuda.memory_reserved(0)/1024**3:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "MJPEP0gmH-ic",
      "metadata": {
        "id": "MJPEP0gmH-ic"
      },
      "source": [
        "\n",
        "## 4. Loading BERT and Tokenizer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "886b9654",
      "metadata": {
        "id": "886b9654"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForMaskedLM: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight']\n",
            "- This IS expected if you are initializing BertForMaskedLM from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForMaskedLM from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0-11): 12 x BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSdpaSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=2, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM, BertForSequenceClassification\n",
        "\n",
        "# Base masked LM model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "mlm_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "mlm_model.to(device)  # Move to GPU\n",
        "mlm_model.eval()  # set to evaluation mode\n",
        "\n",
        "# Sequence classification model (will fine-tune)\n",
        "num_labels = 2  # e.g., binary sentiment\n",
        "clf_model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased', num_labels=num_labels\n",
        ")\n",
        "clf_model.to(device)  # Move to GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3KtpRf10ICDz",
      "metadata": {
        "id": "3KtpRf10ICDz"
      },
      "source": [
        "## 5. Tokenization Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "f8b67b32",
      "metadata": {
        "id": "f8b67b32"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Tokens: ['hello', ',', 'bert', '!']\n",
            "Token IDs: [7592, 1010, 14324, 999]\n"
          ]
        }
      ],
      "source": [
        "text = \"Hello, BERT!\"\n",
        "# Convert to token IDs\n",
        "tokens = tokenizer.tokenize(text)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Token IDs:\", ids)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "S_8G_zqtIFQz",
      "metadata": {
        "id": "S_8G_zqtIFQz"
      },
      "source": [
        "\n",
        "## 6. Masked Language Modeling Example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c3c9e996",
      "metadata": {
        "id": "c3c9e996"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Predicted token for [MASK]: ['paris']\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "# Prepare text with a mask token\n",
        "text = \"The capital of France is [MASK].\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)  # Move to GPU\n",
        "# Run model\n",
        "with torch.no_grad():\n",
        "    outputs = mlm_model(input_ids)\n",
        "    logits = outputs.logits\n",
        "# Locate mask position\n",
        "mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
        "# Predict token\n",
        "mask_logits = logits[0, mask_token_index, :]\n",
        "predicted_token_id = torch.argmax(mask_logits, dim=-1)\n",
        "predicted_token = tokenizer.convert_ids_to_tokens(predicted_token_id.cpu())  # Move back to CPU for tokenizer\n",
        "print(f\"Predicted token for [MASK]: {predicted_token}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "sKFVgumzIIz9",
      "metadata": {
        "id": "sKFVgumzIIz9"
      },
      "source": [
        "## 7. Fine-tuning BERT for Text Classification (Trainer API, not Recommended)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "08b432b5",
      "metadata": {
        "id": "08b432b5"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='250' max='250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [250/250 14:36, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>100</td>\n",
              "      <td>0.026000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>200</td>\n",
              "      <td>0.000100</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
            "  warnings.warn(warn_msg)\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/torch/nn/modules/module.py:1762: FutureWarning: `encoder_attention_mask` is deprecated and will be removed in version 4.55.0 for `BertSdpaSelfAttention.forward`.\n",
            "  return forward_call(*args, **kwargs)\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [63/63 00:41]\n",
              "    </div>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "{'eval_loss': 7.190158794401214e-05, 'eval_runtime': 42.0804, 'eval_samples_per_second': 11.882, 'eval_steps_per_second': 1.497, 'epoch': 1.0}\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Load a sample dataset (IMDb)\n",
        "dataset = load_dataset('imdb', split={'train':'train[:2000]', 'test':'test[:500]'})\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
        "\n",
        "dataset = dataset.map(tokenize_fn, batched=True)\n",
        "dataset.set_format(type='torch', columns=['input_ids','attention_mask','label'])\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=8,\n",
        "    logging_steps=100,\n",
        "      report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=clf_model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test']\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "metrics = trainer.evaluate()   # run a final evaluation pass\n",
        "print(metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "I9yJW8AZINA3",
      "metadata": {
        "id": "I9yJW8AZINA3"
      },
      "source": [
        "## 8. Fine-tuning BERT Manually (Custom Training Loop, Recommended)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "2ab819fa",
      "metadata": {
        "id": "2ab819fa"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Training: 100%|██████████| 250/250 [11:41<00:00,  2.80s/it]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 completed. Loss: 0.7621\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Evaluating: 100%|██████████| 63/63 [00:43<00:00,  1.46it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Loss: 0.7310\n",
            "Accuracy: 0.1800\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load a sample dataset (IMDb)\n",
        "dataset = load_dataset('imdb', split={'train':'train[:2000]', 'test':'test[:500]'})\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
        "\n",
        "dataset = dataset.map(tokenize_fn, batched=True)\n",
        "dataset.set_format(type='torch', columns=['input_ids','attention_mask','label'])\n",
        "# DataLoaders\n",
        "dataset.set_format(type='torch', columns=['input_ids','attention_mask','label'])\n",
        "train_loader = DataLoader(dataset['train'], batch_size=8, shuffle=True)\n",
        "eval_loader = DataLoader(dataset['test'], batch_size=8)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(clf_model.parameters(), lr=5e-5)\n",
        "num_labels = 2  # e.g., binary sentiment\n",
        "clf_model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased', num_labels=num_labels\n",
        ")\n",
        "clf_model.to(device)  # Move to GPU\n",
        "# Training loop\n",
        "clf_model.train()\n",
        "for epoch in range(1):\n",
        "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "        inputs = {\n",
        "            'input_ids': batch['input_ids'].to(clf_model.device),\n",
        "            'attention_mask': batch['attention_mask'].to(clf_model.device)\n",
        "        }\n",
        "        labels = batch['label'].to(clf_model.device)\n",
        "        outputs = clf_model(**inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    print(f\"Epoch {epoch+1} completed. Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluation with Loss and Accuracy\n",
        "clf_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "eval_losses = []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(eval_loader, desc=\"Evaluating\"):\n",
        "        inputs = {\n",
        "            'input_ids': batch['input_ids'].to(clf_model.device),\n",
        "            'attention_mask': batch['attention_mask'].to(clf_model.device)\n",
        "        }\n",
        "        labels = batch['label'].to(clf_model.device)\n",
        "        outputs = clf_model(**inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "        eval_losses.append(loss.item())\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "avg_loss = sum(eval_losses) / len(eval_losses)\n",
        "accuracy = correct / total\n",
        "print(f\"Evaluation Loss: {avg_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "wLU4iyX6IotB",
      "metadata": {
        "id": "wLU4iyX6IotB"
      },
      "source": [
        "\n",
        "## 9. Inference with Fine-tuned Model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "cdcda79f",
      "metadata": {
        "id": "cdcda79f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Text: I absolutely loved this movie!\n",
            "Prediction: negative\n",
            "\n",
            "Text: That was the worst film I've ever seen.\n",
            "Prediction: negative\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# Sample texts\n",
        "texts = [\n",
        "    \"I absolutely loved this movie!\",\n",
        "    \"That was the worst film I've ever seen.\"\n",
        "]\n",
        "# Tokenize and move to GPU\n",
        "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}  # Move all tensors to GPU\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    logits = clf_model(**inputs).logits\n",
        "predictions = torch.argmax(logits, dim=-1).cpu()  # Move back to CPU for processing\n",
        "labels = ['negative', 'positive']\n",
        "for text, pred in zip(texts, predictions):\n",
        "    print(f\"Text: {text}\\nPrediction: {labels[pred]}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "vUgDS4abIszt",
      "metadata": {
        "id": "vUgDS4abIszt"
      },
      "source": [
        "\n",
        "# 🔍 Quiz: Contextualized Word Embeddings in Action\n",
        "\n",
        "## 🧠 Objective\n",
        "You will explore how BERT generates different vector representations for the same word depending on its surrounding context. This tests your understanding of contextualization, a major advantage of BERT over earlier static embeddings like Word2Vec.\n",
        "\n",
        "## 🧪 Task\n",
        "Use the `bert-base-uncased` model to extract the last hidden state vectors of the word \"bank\" in the following two sentences:\n",
        "\n",
        "1. \"He sat down by the bank to enjoy the view of the river.\"\n",
        "2. \"She went to the bank to deposit some cash.\"\n",
        "\n",
        "Then:\n",
        "- Extract the token embedding for the word \"bank\" from both sentences.\n",
        "- Compute the cosine similarity between them.\n",
        "- Discuss why the similarity is (likely) low, even though the surface word is the same.\n",
        "\n",
        "## 📌 Your Code Here\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17ec8ca7",
      "metadata": {
        "id": "17ec8ca7"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "model.to(device)  # Move to GPU\n",
        "model.eval()\n",
        "\n",
        "# Sentences with same word used in different contexts\n",
        "sent1 = \"He sat down by the bank to enjoy the view of the river.\"\n",
        "sent2 = \"She went to the bank to deposit some cash.\"\n",
        "\n",
        "def get_word_embedding(sentence, target_word=\"bank\"):\n",
        "    # TODO: Implement this function\n",
        "    # Hints:\n",
        "    # 1. Tokenize the sentence and move to GPU\n",
        "    # 2. Get model outputs\n",
        "    # 3. Find the index of target_word in tokens\n",
        "    # 4. Extract embedding from last_hidden_state\n",
        "    # Remember to handle device placement!\n",
        "    pass\n",
        "\n",
        "# Get embeddings\n",
        "emb1, tok1 = get_word_embedding(sent1)\n",
        "emb2, tok2 = get_word_embedding(sent2)\n",
        "\n",
        "# Compute similarity\n",
        "similarity = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()\n",
        "\n",
        "print(\"Sentence 1 Tokens:\", tok1)\n",
        "print(\"Sentence 2 Tokens:\", tok2)\n",
        "print(\"Cosine similarity between 'bank' embeddings:\", similarity)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "t97pLoqzI5_V",
      "metadata": {
        "id": "t97pLoqzI5_V"
      },
      "source": [
        "## Discussion\n",
        "### ✅ Insight\n",
        "Question: What value of cosine similarity between these two 'bank' do you observe. If they are equal, the cosine similarity should be 1, otherwise please explain your idea behind this observation.\n",
        "\n",
        "### 💬 Bonus Question\n",
        "Question: Why is it problematic if a model (like Word2Vec) gives similar embeddings for these two \"bank\" instances? When might that lead to incorrect model predictions?\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "Sgcx7i1AJJI2",
      "metadata": {
        "id": "Sgcx7i1AJJI2"
      },
      "source": [
        "## 📚 Complete Implementation (For Reference, Do not see before completing)\n",
        "\n",
        "Below is the complete implementation of the `get_word_embedding` function:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbf6d00e",
      "metadata": {
        "id": "cbf6d00e"
      },
      "outputs": [],
      "source": [
        "# def get_word_embedding(sentence, target_word=\"bank\"):\n",
        "#     \"\"\"\n",
        "#     Extract word embedding for a target word from a sentence using BERT.\n",
        "\n",
        "#     Args:\n",
        "#         sentence (str): Input sentence containing the target word\n",
        "#         target_word (str): The word to extract embedding for\n",
        "\n",
        "#     Returns:\n",
        "#         tuple: (embedding_tensor, token_list)\n",
        "#     \"\"\"\n",
        "#     # Tokenize and move to GPU\n",
        "#     tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
        "#     tokens = {k: v.to(device) for k, v in tokens.items()}\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(**tokens)\n",
        "\n",
        "#     input_ids = tokens[\"input_ids\"][0].cpu()  # Move back to CPU for tokenizer\n",
        "#     token_strs = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "#     # Find index of the word \"bank\"\n",
        "#     # BERT may tokenize it as 'bank' or '##bank', so we do an exact match\n",
        "#     index = token_strs.index(target_word)\n",
        "#     embedding = outputs.last_hidden_state[0, index].cpu()  # Move back to CPU for similarity computation\n",
        "#     return embedding, token_strs\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
