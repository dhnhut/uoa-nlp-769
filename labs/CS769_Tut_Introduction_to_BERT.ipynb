{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Introduction to BERT\n",
        "\n",
        "## Overview\n",
        "\n",
        "In this tutorial, we'll explore the basics of BERT (Bidirectional Encoder Representations from Transformers), learn how to tokenize text, use BERT for masked language modeling, fine-tune it on a classification task (both using Trainer and a manual loop), and run simple inference. At the end, there is a quiz and a discussion section\n",
        "\n",
        "---\n",
        "\n",
        "## 1. Objectives\n",
        "- Understand BERT's architecture and use cases\n",
        "- Load and run a pretrained BERT model with Hugging Face's Transformers\n",
        "- Tokenize input text for BERT\n",
        "- Perform masked language modeling\n",
        "- Fine-tune BERT on a downstream classification task using Trainer\n",
        "- Fine-tune BERT manually with a custom training loop\n",
        "- Run inference with the fine-tuned model\n",
        "- Understand how contextual embeddings differ from previous approaches\n",
        "\n",
        "## 2. Prerequisites\n",
        "- Python 3.7+\n",
        "- `pip install transformers torch datasets`\n",
        "- Basic familiarity with Python and Jupyter notebooks"
      ],
      "metadata": {
        "id": "vy8f3I-0HqGW"
      },
      "id": "vy8f3I-0HqGW"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3. Setup"
      ],
      "metadata": {
        "id": "xMm_opk-H023"
      },
      "id": "xMm_opk-H023"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "959c9993",
      "metadata": {
        "id": "959c9993"
      },
      "outputs": [],
      "source": [
        "# !pip install transformers torch datasets\n",
        "\n",
        "# Set fixed random seed for reproducibility\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "RANDOM_SEED = 42\n",
        "random.seed(RANDOM_SEED)\n",
        "np.random.seed(RANDOM_SEED)\n",
        "torch.manual_seed(RANDOM_SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(RANDOM_SEED)\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "if device.type == 'cuda':\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"Memory allocated: {torch.cuda.memory_allocated(0)/1024**3:.1f} GB\")\n",
        "    print(f\"Memory cached: {torch.cuda.memory_reserved(0)/1024**3:.1f} GB\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 4. Loading BERT and Tokenizer"
      ],
      "metadata": {
        "id": "MJPEP0gmH-ic"
      },
      "id": "MJPEP0gmH-ic"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "886b9654",
      "metadata": {
        "id": "886b9654"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertForMaskedLM, BertForSequenceClassification\n",
        "\n",
        "# Base masked LM model and tokenizer\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "mlm_model = BertForMaskedLM.from_pretrained('bert-base-uncased')\n",
        "mlm_model.to(device)  # Move to GPU\n",
        "mlm_model.eval()  # set to evaluation mode\n",
        "\n",
        "# Sequence classification model (will fine-tune)\n",
        "num_labels = 2  # e.g., binary sentiment\n",
        "clf_model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased', num_labels=num_labels\n",
        ")\n",
        "clf_model.to(device)  # Move to GPU\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 5. Tokenization Example"
      ],
      "metadata": {
        "id": "3KtpRf10ICDz"
      },
      "id": "3KtpRf10ICDz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f8b67b32",
      "metadata": {
        "id": "f8b67b32"
      },
      "outputs": [],
      "source": [
        "text = \"Hello, BERT!\"\n",
        "# Convert to token IDs\n",
        "tokens = tokenizer.tokenize(text)\n",
        "ids = tokenizer.convert_tokens_to_ids(tokens)\n",
        "print(\"Tokens:\", tokens)\n",
        "print(\"Token IDs:\", ids)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 6. Masked Language Modeling Example"
      ],
      "metadata": {
        "id": "S_8G_zqtIFQz"
      },
      "id": "S_8G_zqtIFQz"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c3c9e996",
      "metadata": {
        "id": "c3c9e996"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "# Prepare text with a mask token\n",
        "text = \"The capital of France is [MASK].\"\n",
        "input_ids = tokenizer.encode(text, return_tensors='pt').to(device)  # Move to GPU\n",
        "# Run model\n",
        "with torch.no_grad():\n",
        "    outputs = mlm_model(input_ids)\n",
        "    logits = outputs.logits\n",
        "# Locate mask position\n",
        "mask_token_index = torch.where(input_ids == tokenizer.mask_token_id)[1]\n",
        "# Predict token\n",
        "mask_logits = logits[0, mask_token_index, :]\n",
        "predicted_token_id = torch.argmax(mask_logits, dim=-1)\n",
        "predicted_token = tokenizer.convert_ids_to_tokens(predicted_token_id.cpu())  # Move back to CPU for tokenizer\n",
        "print(f\"Predicted token for [MASK]: {predicted_token}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 7. Fine-tuning BERT for Text Classification (Trainer API, not Recommended)"
      ],
      "metadata": {
        "id": "sKFVgumzIIz9"
      },
      "id": "sKFVgumzIIz9"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "08b432b5",
      "metadata": {
        "id": "08b432b5"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from transformers import TrainingArguments, Trainer\n",
        "\n",
        "# Load a sample dataset (IMDb)\n",
        "dataset = load_dataset('imdb', split={'train':'train[:2000]', 'test':'test[:500]'})\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
        "\n",
        "dataset = dataset.map(tokenize_fn, batched=True)\n",
        "dataset.set_format(type='torch', columns=['input_ids','attention_mask','label'])\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',\n",
        "    num_train_epochs=1,\n",
        "    per_device_train_batch_size=8,\n",
        "    logging_steps=100,\n",
        "      report_to=\"none\"\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=clf_model,\n",
        "    args=training_args,\n",
        "    train_dataset=dataset['train'],\n",
        "    eval_dataset=dataset['test']\n",
        ")\n",
        "\n",
        "trainer.train()\n",
        "metrics = trainer.evaluate()   # run a final evaluation pass\n",
        "print(metrics)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 8. Fine-tuning BERT Manually (Custom Training Loop, Recommended)"
      ],
      "metadata": {
        "id": "I9yJW8AZINA3"
      },
      "id": "I9yJW8AZINA3"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "2ab819fa",
      "metadata": {
        "id": "2ab819fa"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Load a sample dataset (IMDb)\n",
        "dataset = load_dataset('imdb', split={'train':'train[:2000]', 'test':'test[:500]'})\n",
        "\n",
        "# Tokenization function\n",
        "def tokenize_fn(examples):\n",
        "    return tokenizer(examples['text'], padding='max_length', truncation=True)\n",
        "\n",
        "dataset = dataset.map(tokenize_fn, batched=True)\n",
        "dataset.set_format(type='torch', columns=['input_ids','attention_mask','label'])\n",
        "# DataLoaders\n",
        "dataset.set_format(type='torch', columns=['input_ids','attention_mask','label'])\n",
        "train_loader = DataLoader(dataset['train'], batch_size=8, shuffle=True)\n",
        "eval_loader = DataLoader(dataset['test'], batch_size=8)\n",
        "\n",
        "# Optimizer\n",
        "optimizer = AdamW(clf_model.parameters(), lr=5e-5)\n",
        "num_labels = 2  # e.g., binary sentiment\n",
        "clf_model = BertForSequenceClassification.from_pretrained(\n",
        "    'bert-base-uncased', num_labels=num_labels\n",
        ")\n",
        "clf_model.to(device)  # Move to GPU\n",
        "# Training loop\n",
        "clf_model.train()\n",
        "for epoch in range(1):\n",
        "    for batch in tqdm(train_loader, desc=\"Training\"):\n",
        "        inputs = {\n",
        "            'input_ids': batch['input_ids'].to(clf_model.device),\n",
        "            'attention_mask': batch['attention_mask'].to(clf_model.device)\n",
        "        }\n",
        "        labels = batch['label'].to(clf_model.device)\n",
        "        outputs = clf_model(**inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "    print(f\"Epoch {epoch+1} completed. Loss: {loss.item():.4f}\")\n",
        "\n",
        "# Evaluation with Loss and Accuracy\n",
        "clf_model.eval()\n",
        "correct = 0\n",
        "total = 0\n",
        "eval_losses = []\n",
        "with torch.no_grad():\n",
        "    for batch in tqdm(eval_loader, desc=\"Evaluating\"):\n",
        "        inputs = {\n",
        "            'input_ids': batch['input_ids'].to(clf_model.device),\n",
        "            'attention_mask': batch['attention_mask'].to(clf_model.device)\n",
        "        }\n",
        "        labels = batch['label'].to(clf_model.device)\n",
        "        outputs = clf_model(**inputs, labels=labels)\n",
        "        loss = outputs.loss\n",
        "        logits = outputs.logits\n",
        "        eval_losses.append(loss.item())\n",
        "        preds = torch.argmax(logits, dim=-1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "\n",
        "avg_loss = sum(eval_losses) / len(eval_losses)\n",
        "accuracy = correct / total\n",
        "print(f\"Evaluation Loss: {avg_loss:.4f}\")\n",
        "print(f\"Accuracy: {accuracy:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "## 9. Inference with Fine-tuned Model"
      ],
      "metadata": {
        "id": "wLU4iyX6IotB"
      },
      "id": "wLU4iyX6IotB"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdcda79f",
      "metadata": {
        "id": "cdcda79f"
      },
      "outputs": [],
      "source": [
        "# Sample texts\n",
        "texts = [\n",
        "    \"I absolutely loved this movie!\",\n",
        "    \"That was the worst film I've ever seen.\"\n",
        "]\n",
        "# Tokenize and move to GPU\n",
        "inputs = tokenizer(texts, padding=True, truncation=True, return_tensors='pt')\n",
        "inputs = {k: v.to(device) for k, v in inputs.items()}  # Move all tensors to GPU\n",
        "# Predict\n",
        "with torch.no_grad():\n",
        "    logits = clf_model(**inputs).logits\n",
        "predictions = torch.argmax(logits, dim=-1).cpu()  # Move back to CPU for processing\n",
        "labels = ['negative', 'positive']\n",
        "for text, pred in zip(texts, predictions):\n",
        "    print(f\"Text: {text}\\nPrediction: {labels[pred]}\\n\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "# 🔍 Quiz: Contextualized Word Embeddings in Action\n",
        "\n",
        "## 🧠 Objective\n",
        "You will explore how BERT generates different vector representations for the same word depending on its surrounding context. This tests your understanding of contextualization, a major advantage of BERT over earlier static embeddings like Word2Vec.\n",
        "\n",
        "## 🧪 Task\n",
        "Use the `bert-base-uncased` model to extract the last hidden state vectors of the word \"bank\" in the following two sentences:\n",
        "\n",
        "1. \"He sat down by the bank to enjoy the view of the river.\"\n",
        "2. \"She went to the bank to deposit some cash.\"\n",
        "\n",
        "Then:\n",
        "- Extract the token embedding for the word \"bank\" from both sentences.\n",
        "- Compute the cosine similarity between them.\n",
        "- Discuss why the similarity is (likely) low, even though the surface word is the same.\n",
        "\n",
        "## 📌 Your Code Here\n"
      ],
      "metadata": {
        "id": "vUgDS4abIszt"
      },
      "id": "vUgDS4abIszt"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17ec8ca7",
      "metadata": {
        "id": "17ec8ca7"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer, BertModel\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "model = BertModel.from_pretrained(\"bert-base-uncased\")\n",
        "model.to(device)  # Move to GPU\n",
        "model.eval()\n",
        "\n",
        "# Sentences with same word used in different contexts\n",
        "sent1 = \"He sat down by the bank to enjoy the view of the river.\"\n",
        "sent2 = \"She went to the bank to deposit some cash.\"\n",
        "\n",
        "def get_word_embedding(sentence, target_word=\"bank\"):\n",
        "    # TODO: Implement this function\n",
        "    # Hints:\n",
        "    # 1. Tokenize the sentence and move to GPU\n",
        "    # 2. Get model outputs\n",
        "    # 3. Find the index of target_word in tokens\n",
        "    # 4. Extract embedding from last_hidden_state\n",
        "    # Remember to handle device placement!\n",
        "    pass\n",
        "\n",
        "# Get embeddings\n",
        "emb1, tok1 = get_word_embedding(sent1)\n",
        "emb2, tok2 = get_word_embedding(sent2)\n",
        "\n",
        "# Compute similarity\n",
        "similarity = F.cosine_similarity(emb1.unsqueeze(0), emb2.unsqueeze(0)).item()\n",
        "\n",
        "print(\"Sentence 1 Tokens:\", tok1)\n",
        "print(\"Sentence 2 Tokens:\", tok2)\n",
        "print(\"Cosine similarity between 'bank' embeddings:\", similarity)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Discussion\n",
        "### ✅ Insight\n",
        "Question: What value of cosine similarity between these two 'bank' do you observe. If they are equal, the cosine similarity should be 1, otherwise please explain your idea behind this observation.\n",
        "\n",
        "### 💬 Bonus Question\n",
        "Question: Why is it problematic if a model (like Word2Vec) gives similar embeddings for these two \"bank\" instances? When might that lead to incorrect model predictions?\n"
      ],
      "metadata": {
        "id": "t97pLoqzI5_V"
      },
      "id": "t97pLoqzI5_V"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 📚 Complete Implementation (For Reference, Do not see before completing)\n",
        "\n",
        "Below is the complete implementation of the `get_word_embedding` function:"
      ],
      "metadata": {
        "id": "Sgcx7i1AJJI2"
      },
      "id": "Sgcx7i1AJJI2"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cbf6d00e",
      "metadata": {
        "id": "cbf6d00e"
      },
      "outputs": [],
      "source": [
        "# def get_word_embedding(sentence, target_word=\"bank\"):\n",
        "#     \"\"\"\n",
        "#     Extract word embedding for a target word from a sentence using BERT.\n",
        "\n",
        "#     Args:\n",
        "#         sentence (str): Input sentence containing the target word\n",
        "#         target_word (str): The word to extract embedding for\n",
        "\n",
        "#     Returns:\n",
        "#         tuple: (embedding_tensor, token_list)\n",
        "#     \"\"\"\n",
        "#     # Tokenize and move to GPU\n",
        "#     tokens = tokenizer(sentence, return_tensors=\"pt\")\n",
        "#     tokens = {k: v.to(device) for k, v in tokens.items()}\n",
        "\n",
        "#     with torch.no_grad():\n",
        "#         outputs = model(**tokens)\n",
        "\n",
        "#     input_ids = tokens[\"input_ids\"][0].cpu()  # Move back to CPU for tokenizer\n",
        "#     token_strs = tokenizer.convert_ids_to_tokens(input_ids)\n",
        "\n",
        "#     # Find index of the word \"bank\"\n",
        "#     # BERT may tokenize it as 'bank' or '##bank', so we do an exact match\n",
        "#     index = token_strs.index(target_word)\n",
        "#     embedding = outputs.last_hidden_state[0, index].cpu()  # Move back to CPU for similarity computation\n",
        "#     return embedding, token_strs\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    },
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "accelerator": "GPU",
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}