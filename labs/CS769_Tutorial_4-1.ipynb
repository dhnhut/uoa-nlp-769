{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uxmKwizug5mI"
      },
      "source": [
        "# CS769 Tutorial 4\n",
        "\n",
        "\n",
        "<div id=\"Agenda\"></div>\n",
        "\n",
        "## Agenda\n",
        "\n",
        "- [Introduction of Retrieval-augmented generation](#first)\n",
        "- [Implementation of Retrieval-augmented generation](#second)\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gDTzipIjM6po"
      },
      "source": [
        "<div id=\"first\"></div>\n",
        "\n",
        "## 1 Introduction of Retrieval-augmented generation\n",
        "\n",
        "Retrieval-augmented generation (“RAG”) models combine the powers of pretrained dense retrieval (DPR) and sequence-to-sequence models. It is proposed by a [NIPS paprt](https://arxiv.org/abs/2005.11401).\n",
        "\n",
        "**QUIZ**: What is pretrained dense retrieval (DPR)?\n",
        "\n",
        "**QUIZ**: What is sequence-to-sequence model?\n",
        "\n",
        "\n",
        "### 1.1 Work Flow\n",
        "\n",
        "RAG models,\n",
        "- Retrieve documents\n",
        "- Pass them to a seq2seq model\n",
        "- Marginalize to generate outputs\n",
        "\n",
        "\n",
        "The retriever and seq2seq modules are initialized from pretrained models, and fine-tuned jointly, allowing both retrieval and generation to adapt to downstream tasks.\n",
        "\n",
        "\n",
        "For exmaple:\n",
        "\n",
        "  - User Question: *What is NLP?*\n",
        "\n",
        "  - Context:\n",
        "    1. NLP stands for Natural Language Processing, a subfield of artificial intelligence that focuses on the interaction between computers and humans through natural language.\n",
        "    2. It involves the development of algorithms and models that enable machines to understand, interpret, and generate human language in a way that is both meaningful and useful.\n",
        "    3. NLP encompasses a variety of tasks including text analysis, translation, sentiment analysis, and speech recognition.\n",
        "\n",
        "  - The answer might be:\n",
        "\n",
        "    NLP, or Natural Language Processing, is a field of artificial intelligence that focuses on the interaction between computers and human languages. It involves creating algorithms and models that allow computers to understand, interpret, and generate human language. This technology is used in applications such as text analysis, translation, sentiment analysis, and speech recognition, enabling machines to process and respond to human language in a meaningful way.\n",
        "\n",
        "**QUIZ**: Based on the example, can you imagine what is the biggest difference between RAG and normal QA?\n",
        "\n",
        "### 1.2 Retrieval-augmented generation (RAG)\n",
        "\n",
        "RAG idea:\n",
        "\n",
        "![RAG](https://api.wandb.ai/files/cosmo3769/images/projects/38097019/02184762.png)\n",
        "\n",
        "\n",
        "[Back to Agenda](#Agenda)\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tCDzeLeqPE2B"
      },
      "source": [
        "\n",
        "## 2 Instruction of Retrieval-augmented generation\n",
        "\n",
        "### 2.1 Essential Packages\n",
        "\n",
        "We need to install [pytorch](https://pytorch.org/) and may need the Transformers.\n",
        "\n",
        "#### **Step 1**: Activate Conda Environment\n",
        "```\n",
        "# Move to your project directory (Optional)\n",
        "cd </path/to/your/directory>\n",
        "\n",
        "# Activate your environment\n",
        "conda activate <env_name>\n",
        "```\n",
        "\n",
        "#### **Step 2**: Install pytorch\n",
        "```\n",
        "# GPU version and the cuda version is over 11.8\n",
        "conda install pytorch torchvision torchaudio pytorch-cuda=11.8 -c pytorch -c nvidia\n",
        "\n",
        "# CPU version\n",
        "conda install pytorch torchvision torchaudio cpuonly -c pytorch\n",
        "```\n",
        "\n",
        "#### **Step 3**: Install transformers\n",
        "```\n",
        "conda install transformers\n",
        "```\n",
        "\n",
        "To avoid the time in package installization, we can use [Colab](https://colab.research.google.com/) which is already provided these packages.\n",
        "\n",
        "[Back to Agenda](#Agenda)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qz2ydrxVPNb4"
      },
      "source": [
        "### 2.2 RAG Sample\n",
        "\n",
        "First, we need to know the structure of RAG.\n",
        "\n",
        "**QUIZ**: Can you find any error or missing parts in the following RAG pesudo code?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pgnWsZIaPgBE"
      },
      "outputs": [],
      "source": [
        "# Pseudo code for RAG (Retrieval-Augmented Generation)\n",
        "\n",
        "# Function to perform RAG-based question answering\n",
        "def RAG_system(question):\n",
        "\n",
        "    # Step 1: Retrieve relevant documents or passages from the knowledge base\n",
        "    relevant_docs = retrieve_documents(question)\n",
        "\n",
        "    # Step 2: Combine the retrieved documents into a context for the generation model\n",
        "    combined_context = combine_contexts(relevant_docs)\n",
        "\n",
        "    # Step 3: Generate the answer using the generation model with the provided context\n",
        "    generated_answer = generate_answer(question, combined_context)\n",
        "\n",
        "    # Step 4: Return the final generated answer\n",
        "    return generated_answer\n",
        "\n",
        "# Function to retrieve relevant documents based on the question\n",
        "def retrieve_documents(question):\n",
        "    # Assume we have a search index or knowledge base\n",
        "    # Use the question to query and retrieve relevant documents\n",
        "    retrieved_docs = search_knowledge_base(question)\n",
        "    return retrieved_docs\n",
        "\n",
        "# Function to combine the retrieved documents into a single context\n",
        "def combine_contexts(docs):\n",
        "    # Concatenate or select the most relevant parts of the retrieved documents\n",
        "    context = concatenate_docs(docs)\n",
        "    return context\n",
        "\n",
        "# Function to generate an answer based on the question and context\n",
        "def generate_answer(question, context):\n",
        "    # Input the question and combined context into a language generation model\n",
        "    answer = language_model.generate(text=f\"Q: {question}\\nContext: {context}\\nA:\")\n",
        "    return answer\n",
        "\n",
        "# Example usage\n",
        "question = \"What is NLP?\"\n",
        "answer = RAG_system(question)\n",
        "print(answer)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uPRI1hnVPg43"
      },
      "source": [
        "## 3 Implementation of a RAG System\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.1 Craft the RAG System Manually\n",
        "We will use the FLAN-T5 model as the backbone genrative model.\n",
        "\n",
        "We will implement the functions in the above sample code correspondingly."
      ],
      "metadata": {
        "id": "3cpUE9vpDwzx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random, numpy as np, torch\n",
        "\n",
        "seed = 42\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "torch.cuda.manual_seed_all(seed)  # if using GPU"
      ],
      "metadata": {
        "id": "GDfkfc6AEUOb"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "# Import required libraries (all pre-installed in Colab)\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    pipeline\n",
        ")\n",
        "import torch\n",
        "from typing import List, Tuple\n",
        "\n",
        "# Set device\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"Using device: {device}\")\n",
        "\n",
        "# ============================================\n",
        "# SETUP: Initialize Components\n",
        "# ============================================\n",
        "\n",
        "print(\"Loading model components... This may take a minute on first run.\")\n",
        "\n",
        "# We'll use FLAN-T5 which is excellent for question answering\n",
        "# and works well with context augmentation\n",
        "model_name = \"google/flan-t5-base\"  # You can also try \"google/flan-t5-small\" for faster/lighter\n",
        "\n",
        "# Load tokenizer and model\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "\n",
        "print(\"Model loaded successfully!\")\n",
        "\n",
        "# ============================================\n",
        "# RAG SYSTEM IMPLEMENTATION\n",
        "# ============================================\n",
        "\n",
        "def RAG_system(question: str, knowledge_base: List[str] = None) -> str:\n",
        "    \"\"\"\n",
        "    Main RAG function that performs retrieval-augmented generation\n",
        "    Following the pseudo-code structure exactly\n",
        "\n",
        "    Args:\n",
        "        question: The input question to answer\n",
        "        knowledge_base: Optional list of documents for retrieval\n",
        "\n",
        "    Returns:\n",
        "        Generated answer string\n",
        "    \"\"\"\n",
        "\n",
        "    if knowledge_base:\n",
        "        # Step 1: Retrieve relevant documents from knowledge base\n",
        "        relevant_docs = retrieve_documents(question, knowledge_base)\n",
        "\n",
        "        # Step 2: Combine retrieved documents into context\n",
        "        combined_context = combine_contexts(relevant_docs)\n",
        "\n",
        "        # Step 3: Generate answer using question and context\n",
        "        generated_answer = generate_answer(question, combined_context)\n",
        "    else:\n",
        "        # Generate answer without retrieval (using model's parametric knowledge)\n",
        "        generated_answer = generate_answer(question, \"\")\n",
        "\n",
        "    # Step 4: Return the final generated answer\n",
        "    return generated_answer\n",
        "\n",
        "\n",
        "def retrieve_documents(question: str, knowledge_base: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Retrieve relevant documents based on the question\n",
        "    Uses simple TF-IDF-like scoring for demonstration\n",
        "\n",
        "    Args:\n",
        "        question: Input question\n",
        "        knowledge_base: List of document strings\n",
        "\n",
        "    Returns:\n",
        "        List of relevant documents (top 3)\n",
        "    \"\"\"\n",
        "    if not knowledge_base:\n",
        "        return []\n",
        "\n",
        "    # Simple retrieval using word overlap scoring\n",
        "    question_words = set(question.lower().split())\n",
        "\n",
        "    # Score each document\n",
        "    doc_scores = []\n",
        "    for doc in knowledge_base:\n",
        "        doc_words = set(doc.lower().split())\n",
        "        # Calculate overlap score\n",
        "        common_words = question_words.intersection(doc_words)\n",
        "        score = len(common_words)\n",
        "        # Bonus for exact phrase matches\n",
        "        if any(word in doc.lower() for word in question.lower().split() if len(word) > 3):\n",
        "            score += 1\n",
        "        doc_scores.append((score, doc))\n",
        "\n",
        "    # Sort by score and get top 3\n",
        "    doc_scores.sort(reverse=True, key=lambda x: x[0])\n",
        "\n",
        "    # Return top documents with non-zero scores\n",
        "    relevant_docs = [doc for score, doc in doc_scores[:3] if score > 0]\n",
        "\n",
        "    print(f\"Retrieved {len(relevant_docs)} relevant documents\")\n",
        "    return relevant_docs\n",
        "\n",
        "\n",
        "def combine_contexts(docs: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    Combine the retrieved documents into a single context\n",
        "\n",
        "    Args:\n",
        "        docs: List of document strings\n",
        "\n",
        "    Returns:\n",
        "        Combined context string\n",
        "    \"\"\"\n",
        "    if not docs:\n",
        "        return \"\"\n",
        "\n",
        "    # Concatenate documents with separator\n",
        "    context = \" \".join(docs)\n",
        "\n",
        "    # Truncate if too long (to fit in model's input limit)\n",
        "    max_words = 300  # Keep context reasonable\n",
        "    words = context.split()\n",
        "    if len(words) > max_words:\n",
        "        context = \" \".join(words[:max_words])\n",
        "\n",
        "    return context\n",
        "\n",
        "\n",
        "def generate_answer(question: str, context: str) -> str:\n",
        "    \"\"\"\n",
        "    Generate an answer based on the question and context\n",
        "    Using the language model with appropriate prompting\n",
        "\n",
        "    Args:\n",
        "        question: Input question\n",
        "        context: Combined context from retrieved documents\n",
        "\n",
        "    Returns:\n",
        "        Generated answer string\n",
        "    \"\"\"\n",
        "    # Create prompt based on whether we have context\n",
        "    if context:\n",
        "        # RAG-style prompt with context\n",
        "        prompt = f\"\"\"Answer the question based on the following context.\n",
        "\n",
        "Context: {context}\n",
        "\n",
        "Question: {question}\n",
        "\n",
        "Answer:\"\"\"\n",
        "    else:\n",
        "        # Direct question answering without context\n",
        "        prompt = f\"Question: {question}\\nAnswer:\"\n",
        "\n",
        "    # Tokenize input\n",
        "    inputs = tokenizer(\n",
        "        prompt,\n",
        "        return_tensors=\"pt\",\n",
        "        max_length=512,\n",
        "        truncation=True,\n",
        "        padding=True\n",
        "    ).to(device)\n",
        "\n",
        "    # Generate answer\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            attention_mask=inputs[\"attention_mask\"],\n",
        "            max_length=150,\n",
        "            min_length=10,\n",
        "            temperature=0.7,\n",
        "            do_sample=False,  # Deterministic for consistency\n",
        "            num_beams=3,\n",
        "            early_stopping=True\n",
        "        )\n",
        "\n",
        "    # Decode and return answer\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# HELPER FUNCTIONS FOR DEMONSTRATION\n",
        "# ============================================\n",
        "\n",
        "def search_knowledge_base(question: str, kb: List[str]) -> List[str]:\n",
        "    \"\"\"\n",
        "    Wrapper function matching the pseudo-code's search_knowledge_base\n",
        "    \"\"\"\n",
        "    return retrieve_documents(question, kb)\n",
        "\n",
        "\n",
        "def concatenate_docs(docs: List[str]) -> str:\n",
        "    \"\"\"\n",
        "    Wrapper function matching the pseudo-code's concatenate_docs\n",
        "    \"\"\"\n",
        "    return combine_contexts(docs)\n",
        "\n",
        "\n",
        "# ============================================\n",
        "# EXAMPLE KNOWLEDGE BASE\n",
        "# ============================================\n",
        "\n",
        "# Create a comprehensive knowledge base for demonstration\n",
        "default_knowledge_base = [\n",
        "    \"Natural Language Processing (NLP) is a subfield of artificial intelligence that focuses on the interaction between computers and human language. It combines computational linguistics with machine learning and deep learning.\",\n",
        "\n",
        "    \"NLP enables computers to understand, interpret, and generate human language in a valuable way. Common applications include machine translation, sentiment analysis, and chatbots.\",\n",
        "\n",
        "    \"Key NLP tasks include tokenization, part-of-speech tagging, named entity recognition, sentiment analysis, text classification, and question answering.\",\n",
        "\n",
        "    \"Transformers are a neural network architecture introduced in 2017 that revolutionized NLP. They use self-attention mechanisms to process sequential data more effectively than RNNs or LSTMs.\",\n",
        "\n",
        "    \"BERT (Bidirectional Encoder Representations from Transformers) is a pre-trained language model developed by Google that achieves state-of-the-art results on many NLP tasks.\",\n",
        "\n",
        "    \"GPT (Generative Pre-trained Transformer) is a series of language models developed by OpenAI that excel at text generation and have been scaled to billions of parameters.\",\n",
        "\n",
        "    \"RAG (Retrieval-Augmented Generation) combines information retrieval with text generation, allowing models to access external knowledge bases for more accurate and factual responses.\",\n",
        "\n",
        "    \"Word embeddings like Word2Vec and GloVe represent words as dense vectors, capturing semantic relationships between words in a continuous vector space.\",\n",
        "\n",
        "    \"Attention mechanisms allow models to focus on relevant parts of the input when producing output, significantly improving performance on tasks like machine translation.\",\n",
        "\n",
        "    \"Transfer learning in NLP involves pre-training models on large text corpora and then fine-tuning them for specific downstream tasks, reducing the need for task-specific training data.\"\n",
        "]\n",
        "\n",
        "# ============================================\n",
        "# EXAMPLE USAGE\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"RAG SYSTEM EXAMPLES\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "# Example 1: Question with retrieval from knowledge base\n",
        "print(\"\\n--- Example 1: RAG with Knowledge Base ---\")\n",
        "question1 = \"What is NLP?\"\n",
        "print(f\"Question: {question1}\")\n",
        "answer1 = RAG_system(question1, knowledge_base=default_knowledge_base)\n",
        "print(f\"Answer: {answer1}\")\n",
        "\n",
        "# Example 2: Question about transformers\n",
        "print(\"\\n--- Example 2: Question about Transformers ---\")\n",
        "question2 = \"What are transformers in NLP?\"\n",
        "print(f\"Question: {question2}\")\n",
        "answer2 = RAG_system(question2, knowledge_base=default_knowledge_base)\n",
        "print(f\"Answer: {answer2}\")\n",
        "\n",
        "# Example 3: Question about RAG itself\n",
        "print(\"\\n--- Example 3: Question about RAG ---\")\n",
        "question3 = \"What is RAG and how does it work?\"\n",
        "print(f\"Question: {question3}\")\n",
        "answer3 = RAG_system(question3, knowledge_base=default_knowledge_base)\n",
        "print(f\"Answer: {answer3}\")\n",
        "\n",
        "# Example 4: Direct question without knowledge base\n",
        "print(\"\\n--- Example 4: Direct Question (No Retrieval) ---\")\n",
        "question4 = \"What is the capital of France?\"\n",
        "print(f\"Question: {question4}\")\n",
        "answer4 = RAG_system(question4)  # No knowledge base provided\n",
        "print(f\"Answer: {answer4}\")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yFdVh1rX3WIt",
        "outputId": "9cffb4f3-480c-4751-ffb0-fdeeafa02c69"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Loading model components... This may take a minute on first run.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully!\n",
            "\n",
            "==================================================\n",
            "RAG SYSTEM EXAMPLES\n",
            "==================================================\n",
            "\n",
            "--- Example 1: RAG with Knowledge Base ---\n",
            "Question: What is NLP?\n",
            "Retrieved 3 relevant documents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: a subfield of artificial intelligence that focuses on the interaction between computers and human language\n",
            "\n",
            "--- Example 2: Question about Transformers ---\n",
            "Question: What are transformers in NLP?\n",
            "Retrieved 3 relevant documents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: a neural network architecture introduced in 2017 that revolutionized\n",
            "\n",
            "--- Example 3: Question about RAG ---\n",
            "Question: What is RAG and how does it work?\n",
            "Retrieved 3 relevant documents\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The following generation flags are not valid and may be ignored: ['temperature']. Set `TRANSFORMERS_VERBOSITY=info` for more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Answer: RAG (Retrieval-Augmented Generation) combines information retrieval with text generation\n",
            "\n",
            "--- Example 4: Direct Question (No Retrieval) ---\n",
            "Question: What is the capital of France?\n",
            "Answer: saône-by-seine\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## 3.2 Try a Pre-trained RAG Model with Built in Retriver and Generator\n",
        "In the hugging face, there are a lot of models.\n",
        "\n",
        "You can find [here](https://huggingface.co/models?search=rag).\n",
        "\n",
        "Following is one of the example model and usage."
      ],
      "metadata": {
        "id": "GevqkquwDP74"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# ============================================\n",
        "# STEP 1: Run this installation first\n",
        "# ============================================\n",
        "!pip install faiss-cpu -q\n",
        "print(\"faiss-cpu installed!\")\n",
        "import faiss; print(faiss.__version__)\n",
        "!pip install datasets==2.19.0\n",
        "!pip -q install \"numpy<2.0\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ABL89cIi96U-",
        "outputId": "e34b2127-8e32-4de9-c3c1-d3e2866aec08"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "faiss-cpu installed!\n",
            "1.11.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "Facebook RAG Model - Direct Implementation\n",
        "\"\"\"\n",
        "\n",
        "# ============================================\n",
        "# STEP 2: After installation, run this\n",
        "# (If error, restart runtime then run)\n",
        "# ============================================\n",
        "\n",
        "from transformers import RagTokenizer, RagTokenForGeneration, RagRetriever\n",
        "import torch\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using: {device}\\n\")\n",
        "\n",
        "# Load Facebook's RAG model\n",
        "model_name = \"facebook/rag-token-nq\"\n",
        "\n",
        "print(\"Loading RAG components...\")\n",
        "\n",
        "# Load tokenizer\n",
        "tokenizer = RagTokenizer.from_pretrained(model_name)\n",
        "print(\"✓ Tokenizer loaded\")\n",
        "\n",
        "# Load retriever\n",
        "retriever = RagRetriever.from_pretrained(\n",
        "    model_name,\n",
        "    index_name=\"exact\",\n",
        "    use_dummy_dataset=True\n",
        ")\n",
        "print(\"✓ Retriever loaded\")\n",
        "\n",
        "# Load model with retriever\n",
        "model = RagTokenForGeneration.from_pretrained(\n",
        "    model_name,\n",
        "    retriever=retriever\n",
        ").to(device)\n",
        "print(\"✓ RAG model loaded\\n\")\n",
        "\n",
        "def ask_rag(question):\n",
        "    \"\"\"Ask question using Facebook's RAG model\"\"\"\n",
        "    inputs = tokenizer(question, return_tensors=\"pt\").to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model.generate(\n",
        "            input_ids=inputs[\"input_ids\"],\n",
        "            max_length=1000\n",
        "\n",
        "        )\n",
        "\n",
        "    answer = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "    return answer\n",
        "\n",
        "# Test it\n",
        "print(\"=\"*50)\n",
        "print(\"TESTING FACEBOOK RAG MODEL\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "questions = [\n",
        "    \"What is the capital of France?\",\n",
        "    \"Who invented the telephone?\",\n",
        "    \"What is data science\",\n",
        "    \"When was the moon landing?\",\n",
        "]\n",
        "\n",
        "for q in questions:\n",
        "    print(f\"\\nQ: {q}\")\n",
        "    answer = ask_rag(q)\n",
        "    print(f\"A: {answer}\")\n",
        "\n",
        "print(\"\\n✅ Facebook RAG model working!\")\n",
        "print(\"Usage: answer = ask_rag('your question')\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QwxUDTE054J1",
        "outputId": "d032a7a0-d209-4b17-8dc6-3e4a97894aca"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using: cuda\n",
            "\n",
            "Loading RAG components...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'BartTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'BartTokenizerFast'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Tokenizer loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'DPRQuestionEncoderTokenizerFast'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'BartTokenizer'.\n",
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'RagTokenizer'. \n",
            "The class this function is called from is 'BartTokenizerFast'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ Retriever loaded\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of the model checkpoint at facebook/rag-token-nq were not used when initializing RagTokenForGeneration: ['rag.question_encoder.question_encoder.bert_model.pooler.dense.bias', 'rag.question_encoder.question_encoder.bert_model.pooler.dense.weight']\n",
            "- This IS expected if you are initializing RagTokenForGeneration from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RagTokenForGeneration from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✓ RAG model loaded\n",
            "\n",
            "==================================================\n",
            "TESTING FACEBOOK RAG MODEL\n",
            "==================================================\n",
            "\n",
            "Q: What is the capital of France?\n",
            "A:  city of paris\n",
            "\n",
            "Q: Who invented the telephone?\n",
            "A:  alexander graham bell\n",
            "\n",
            "Q: What is data science\n",
            "A:  analysis of data\n",
            "\n",
            "Q: When was the moon landing?\n",
            "A:  july 20 , 1969\n",
            "\n",
            "✅ Facebook RAG model working!\n",
            "Usage: answer = ask_rag('your question')\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}