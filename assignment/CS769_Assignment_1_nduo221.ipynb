{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVx0taBG01Fd"
      },
      "source": [
        "# COMPSCI 769 - Assignment 1\n",
        "\n",
        "**Total Points:** 30  \n",
        "**Runtime:** Colab **T4 GPU**  \n",
        "**Dependencies:** `transformers`, `datasets`, `peft`, `accelerate`, `sentence-transformers`, `evaluate`, `scipy`, `gensim`\n",
        "**Objectives:**\n",
        "\n",
        "\n",
        "1.   Assessing your understanding on the difference between static word embeddings and contextual word embeddings\n",
        "2.   Assessing your understanding on different token feature fusion strategies of BERT\n",
        "3. Assessing your ability of conducting instruction fine-tuning on pre-trained generative language models and understanding on its corresponding effects.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**Note:** There is no need to install dependencies other than the gensim. Google Colab has already setup this environment for you. Enforcing downloading dependencies of newer versions may airse package conflicts. Just follow the `pip install` commands already provided in this assignment.\n",
        "\n",
        "**Please keep the console outputs, and do not clear them for marking easiness**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_fDFEIPB2UY"
      },
      "source": [
        "**Name:** Nhut Hoang Duong\n",
        "\n",
        "**UPI:** nduo221\n",
        "\n",
        "**Student ID:** 804763240"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "uadbUKksw-YR"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'mps'"
            ]
          },
          "execution_count": 1,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ✅ Setup\n",
        "import os, random, numpy as np, torch\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "DEVICE = \"cuda\" \n",
        "if torch.cuda.is_available():\n",
        "    DEVICE = \"cuda\"\n",
        "elif torch.backends.mps.is_available():\n",
        "    DEVICE = \"mps\"\n",
        "else:\n",
        "    DEVICE = \"cpu\"\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3BCO-X82XeB"
      },
      "source": [
        "# Part A — Static (Word2Vec) vs. Contextual (BERT) Token Representations (9 pts)\n",
        "\n",
        "**Objective.** Show that **static** word vectors (one vector per token type) cannot disambiguate polysemy (e.g., bank), while **contextual** token embeddings group occurrences by sense (e.g., riverbank vs. financial).\n",
        "\n",
        "**Code Implementation (6 pts):** Implement `contextual_token_vec(sentence, word=\"bank\")`.\n",
        "\n",
        "**Discussion (3 pt):** Explain your observed similarity patterns and why they occur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pq7bokg7xEjv"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/requests/__init__.py:86: RequestsDependencyWarning: Unable to find acceptable character detection dependency (chardet or charset_normalizer).\n",
            "  warnings.warn(\n",
            "/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# !pip install gensim\n",
        "import torch, numpy as np, itertools\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "sentences = [\n",
        "    \"Willows lined the bank of the stream.\",\n",
        "    \"I opened a new bank account yesterday.\",\n",
        "    \"The fisherman sat quietly by the bank.\",\n",
        "    \"The central bank raised interest rates.\"\n",
        "]\n",
        "target_word = \"bank\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_-5PgCL3IUL"
      },
      "source": [
        "**Note:** *After installing the gensim package, please restart the session following the instructions printed in the console.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "KFEfk7yI0-NS"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "array([[1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1.],\n",
              "       [1., 1., 1., 1.]])"
            ]
          },
          "execution_count": 3,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# ---- Static baseline: small Word2Vec trained quickly on a slice of text8 ----\n",
        "import torch, numpy as np, itertools\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Load a manageable slice for speed\n",
        "text8 = api.load(\"text8\")                          # generator of tokenized sentences\n",
        "sentences_w2v = [s for _, s in zip(range(50_000), text8)]  # ~50k lines\n",
        "\n",
        "# Train a compact skip-gram Word2Vec\n",
        "w2v = Word2Vec(\n",
        "    sentences=sentences_w2v,\n",
        "    vector_size=100, window=5, min_count=5, workers=2,\n",
        "    sg=1, epochs=3\n",
        ")\n",
        "\n",
        "# One static vector for the token type \"bank\"\n",
        "w2v_bank = torch.tensor(w2v.wv[target_word]).float()\n",
        "\n",
        "def cosine(a, b):\n",
        "    return torch.nn.functional.cosine_similarity(a, b, dim=-1)\n",
        "\n",
        "# Pairwise similarities under the SAME static \"bank\" vector\n",
        "reps_static = torch.stack([w2v_bank for _ in sentences])  # [N, D]\n",
        "pairwise_static = np.zeros((len(sentences), len(sentences)))\n",
        "for i, j in itertools.product(range(len(sentences)), range(len(sentences))):\n",
        "    pairwise_static[i, j] = float(cosine(reps_static[i], reps_static[j]))\n",
        "pairwise_static  # expected ~ all ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokenized: tensor([[  101, 11940,  2015,  7732,  1996,  2924,  1997,  1996,  5460,  1012,\n",
            "           102]], device='mps:0')\n",
            "tokens: ['[CLS]', 'willow', '##s', 'lined', 'the', 'bank', 'of', 'the', 'stream', '.', '[SEP]']\n",
            "target_position: 5\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([-3.3650e-02, -4.0651e-01, -4.2578e-01,  2.5236e-02, -5.5180e-01,\n",
              "         2.5169e-01,  1.5093e-01,  2.1321e+00,  2.0032e-01, -1.8318e-01,\n",
              "         9.6993e-01, -1.5982e-01,  3.9372e-01,  2.3406e-01, -7.8746e-01,\n",
              "         3.5004e-01, -1.1001e-01,  1.9852e-01,  5.8292e-01,  3.1892e-01,\n",
              "         5.4507e-01, -9.3500e-02, -7.2563e-02,  1.2025e+00,  2.7306e-01,\n",
              "         7.0967e-01,  7.6448e-01,  6.3141e-02, -1.5365e-01,  3.2747e-01,\n",
              "         1.1737e+00,  4.5297e-01, -1.8828e-01, -2.4276e-01, -7.5355e-01,\n",
              "         4.0795e-01,  9.3041e-02, -6.1755e-01, -8.8286e-01,  6.5056e-01,\n",
              "        -8.0550e-01, -4.7044e-01, -5.5397e-01,  9.4918e-01,  5.8524e-01,\n",
              "         3.8790e-01,  2.3710e-01, -2.9180e-01,  3.7618e-01,  4.0200e-02,\n",
              "        -5.0435e-01,  4.3004e-01, -1.3380e-01, -6.5136e-01,  1.5740e-01,\n",
              "         2.8018e-01, -7.4569e-02, -1.0982e+00, -3.6560e-01,  4.5398e-01,\n",
              "         6.6386e-01,  1.5240e-01,  6.5575e-01, -1.8351e-01,  1.0039e-01,\n",
              "         2.8807e-01, -8.6597e-01,  6.7807e-02, -3.2195e-01,  2.1830e-01,\n",
              "         5.6792e-02,  9.4467e-01, -1.3555e-01,  3.1855e-01, -1.3836e-02,\n",
              "        -2.5266e-01, -2.3192e-01,  3.5237e-01, -6.3095e-01, -3.0629e-01,\n",
              "        -1.5050e-01, -1.0360e-01, -1.2172e-01,  8.9118e-01,  4.4707e-01,\n",
              "        -1.1257e-01,  2.6297e-01,  1.2080e-01, -5.3734e-01,  3.7415e-01,\n",
              "        -9.6264e-01, -6.7438e-02, -7.0468e-02, -4.5493e-01, -1.7880e-01,\n",
              "         4.7293e-01, -4.9780e-01, -4.6623e-01,  4.1006e-01,  6.8633e-01,\n",
              "         3.7071e-01, -1.0272e+00,  6.1310e-01, -1.5833e-02, -1.0524e+00,\n",
              "        -3.2809e-01,  3.1110e-01, -4.1060e-01, -8.0015e-02,  7.9556e-01,\n",
              "         5.6405e-01, -6.3066e-02, -4.8341e-01, -5.0707e-01, -4.7656e-01,\n",
              "         4.0763e-01,  1.9160e-03, -4.9274e-01, -6.4527e-01, -6.4697e-01,\n",
              "         8.9347e-01, -3.2208e-01, -1.2682e-01,  5.0745e-01,  1.7657e-01,\n",
              "        -2.0528e-01, -3.0345e-01,  1.1739e+00, -3.0293e-01,  1.8206e-01,\n",
              "         5.4234e-01, -2.1361e-01, -1.6949e-01, -9.9614e-01,  5.2453e-01,\n",
              "         2.3059e-01,  2.6342e-01, -1.0750e+00, -3.5049e-01,  5.0893e-01,\n",
              "        -2.6756e-01, -3.1099e-01,  3.7331e-01,  7.5143e-02,  8.5766e-01,\n",
              "         1.4519e-01,  5.8945e-01, -8.7385e-01, -4.2500e-01,  1.2535e-01,\n",
              "         3.1577e-01, -4.3116e-01, -2.4073e-01, -4.2181e-01,  2.8363e-01,\n",
              "        -2.2208e-01,  7.2798e-01,  9.4000e-01, -2.5868e-02,  2.4448e-01,\n",
              "         1.1396e-01,  8.3606e-02, -1.0731e-01,  5.1599e-01,  3.9863e-01,\n",
              "        -6.7436e-01,  3.9942e-01,  7.5758e-02, -8.6839e-01,  6.8077e-01,\n",
              "        -4.9499e-01, -9.3289e-02,  6.7560e-01, -8.0471e-01,  6.5382e-01,\n",
              "         1.1274e-01, -5.5386e-02, -9.3595e-01, -6.1834e-01,  4.8780e-01,\n",
              "        -2.1864e-01,  3.1019e-01,  1.4599e+00, -5.1955e-02, -2.3052e-01,\n",
              "        -4.0485e-01,  2.6276e-01, -4.0800e-01,  1.5135e-01, -7.1576e-02,\n",
              "        -7.6812e-01, -8.7204e-01, -1.0872e+00, -2.2461e-01, -4.5268e-01,\n",
              "        -1.6397e+00, -2.6510e-01, -4.3599e-01, -4.8262e-01, -9.8305e-01,\n",
              "        -4.2125e-02, -3.4106e-01,  1.8762e-01,  4.6910e-01, -6.5010e-01,\n",
              "         2.0357e-01, -9.3781e-02, -5.5145e-01, -6.3355e-01, -3.4297e-01,\n",
              "        -2.4235e-01, -3.4630e-01,  2.5104e-01,  4.5082e-01, -5.8260e-01,\n",
              "         9.0295e-03, -1.7000e-01, -8.7266e-01,  4.5464e-01,  2.4373e-01,\n",
              "        -1.4231e-01,  2.9873e-01,  9.6627e-03,  2.4426e-01,  4.4497e-01,\n",
              "         1.0972e+00, -9.2990e-02, -2.3333e-02, -3.2084e-01,  1.0924e+00,\n",
              "        -2.4597e-01,  4.2402e-01,  1.6558e+00, -2.4915e-01, -1.6102e-01,\n",
              "         2.0143e-01, -1.0390e+00, -3.9221e-02,  1.8651e-01,  3.8127e-01,\n",
              "        -1.9785e-01, -3.9039e-01,  6.1147e-01, -2.4538e-01,  5.9471e-01,\n",
              "         2.0853e-02,  6.0827e-01,  2.5642e-01, -2.1968e-01, -9.1292e-01,\n",
              "        -4.9082e-01,  3.9664e-01, -4.1652e-01, -3.7631e-01,  4.9717e-01,\n",
              "        -9.5425e-03,  3.0109e-02,  1.9562e-01, -7.9706e-01,  5.4130e-01,\n",
              "         9.6577e-01,  8.2430e-01, -1.2440e-01,  3.8172e-01, -2.9201e-02,\n",
              "        -9.1380e-01,  2.9440e-01,  5.7856e-01,  2.0451e-01,  1.6155e-01,\n",
              "        -1.7315e-01,  8.1182e-01, -8.1559e-02,  7.7253e-01, -1.0940e+00,\n",
              "        -4.4609e-01,  1.0294e-01, -3.6598e-01,  1.6639e-01, -1.0722e+00,\n",
              "         9.7376e-01,  5.0554e-01, -5.5275e-01,  1.6589e-01, -2.6718e-01,\n",
              "        -9.9151e-01, -3.9887e-01,  1.4617e+00, -4.8335e-02, -3.7824e-01,\n",
              "         1.7567e-01,  1.3115e+00, -6.1020e-01,  4.6990e-01,  5.4750e-01,\n",
              "         5.8060e-01, -2.5490e-02, -3.8857e-01,  1.0787e+00, -7.1861e-01,\n",
              "         4.3728e-01,  1.0255e-01,  8.7091e-01,  1.8248e-02,  2.6321e-01,\n",
              "        -1.0852e-01,  7.4571e-01,  5.1567e-01, -3.4571e+00,  1.0626e+00,\n",
              "         5.8491e-01, -2.1733e-01,  1.4384e+00, -7.4224e-01,  1.1661e-01,\n",
              "        -1.4240e-01, -1.1640e+00,  2.4697e-01,  5.8237e-01,  3.3619e-02,\n",
              "         5.8476e-01,  4.1702e-01,  3.1431e-01, -5.8722e-01,  2.9485e-01,\n",
              "         2.0652e-01, -5.1272e-02, -1.4136e-01, -5.9681e-01,  1.5916e-01,\n",
              "         5.7804e-01, -1.5090e-01,  5.1575e-01,  5.0108e-01, -4.2252e-01,\n",
              "         2.1219e-01, -2.4734e-01,  1.1661e-02, -3.7813e-01, -5.8139e-01,\n",
              "         3.7130e-01, -6.5151e-01, -1.6401e-01, -4.1938e-01,  3.7674e-01,\n",
              "        -3.3368e-01,  3.0540e-01, -2.4917e-02, -2.7667e-01, -1.0207e-01,\n",
              "        -9.4762e-03, -2.4608e-01,  1.6138e+00, -3.3900e-01,  3.5609e-01,\n",
              "        -2.8168e-01,  3.5914e-01, -3.9745e-02,  2.6737e-01, -4.2887e-01,\n",
              "         7.0272e-01, -3.3848e-01, -9.8737e-01, -4.7566e-01,  4.7908e-01,\n",
              "        -3.4766e-01,  3.3592e-01, -2.5377e-01, -3.1627e-01, -5.5011e-02,\n",
              "        -4.0424e-01, -5.2665e-02,  7.1742e-01, -4.8995e-01, -1.1240e+00,\n",
              "        -1.3675e+00, -5.1154e-01,  3.1804e-01,  7.7537e-02,  7.4944e-01,\n",
              "        -6.6775e-01, -9.7025e-01, -8.3161e-01, -6.6485e-01,  5.5921e-01,\n",
              "        -3.0810e-01, -2.1176e-01, -3.6281e-01, -4.8769e-01, -2.6981e-01,\n",
              "         2.7224e-02, -1.3699e+00,  3.8053e-01, -4.9626e-01, -6.7044e-02,\n",
              "        -5.3387e-01, -6.5326e-01, -5.7185e-01,  3.3529e-01, -1.5797e-01,\n",
              "         2.2009e-02, -5.4694e-01, -9.3822e-01,  5.5392e-02, -2.5016e-01,\n",
              "        -3.7309e-01,  3.6727e-01, -5.9432e-01,  7.0575e-01,  3.8379e-01,\n",
              "        -3.2323e-01,  3.1156e-01, -5.3955e-01,  3.9768e-01, -1.0546e+00,\n",
              "        -2.5859e-01, -5.3190e-01,  6.9884e-01,  4.9231e-01, -8.8081e-01,\n",
              "         1.0518e+00, -6.3104e-01, -9.4266e-01,  3.8989e-01,  1.2509e-01,\n",
              "         2.7510e-01,  2.3907e-01, -2.2967e-01, -2.3268e-01,  6.5359e-01,\n",
              "        -6.2354e-01, -1.5506e-02,  9.7867e-01,  1.0460e+00,  2.5861e-01,\n",
              "        -1.9652e-02,  1.9386e-01, -3.6985e-01,  7.8933e-01,  4.8699e-02,\n",
              "         4.5495e-01,  5.8108e-01, -9.7131e-02, -2.1864e-01, -6.1419e-01,\n",
              "        -1.1316e+00, -2.6758e-03,  4.6398e-01, -1.2170e-01,  4.2594e-01,\n",
              "         2.6388e-01, -9.4846e-02,  4.3009e-01,  3.8271e-01, -7.7927e-01,\n",
              "        -5.1513e-02, -2.8232e-01, -9.0887e-02, -3.8763e-01,  2.7362e-01,\n",
              "        -3.8852e-02, -4.4538e-01,  2.2724e-01, -9.7937e-02,  9.6344e-01,\n",
              "        -2.9076e-01,  3.0147e-01, -6.9459e-01,  2.6012e-01,  2.7278e-01,\n",
              "         1.3485e+00,  1.1920e+00, -1.0173e+00,  5.1761e-01, -5.0508e-01,\n",
              "        -2.1647e-02, -7.9820e-01, -3.3091e-02, -9.4193e-01, -3.8080e-01,\n",
              "         6.4659e-01,  7.0168e-01, -4.0507e-01,  1.3283e+00, -7.6965e-01,\n",
              "        -7.0733e-03, -2.0146e-01, -4.6842e-01,  4.9436e-02,  2.2469e-01,\n",
              "         8.1308e-02,  1.8217e-01,  7.0949e-03, -2.5150e-01, -4.0696e-02,\n",
              "         8.7102e-02, -3.3989e-01,  8.3455e-02,  3.5176e-01, -2.2620e-01,\n",
              "        -6.3576e-02, -6.4046e-01, -5.8860e-01, -1.0963e+00,  7.4281e-01,\n",
              "        -3.1208e-02, -1.8309e-01,  1.8764e-01, -2.9450e-01, -4.3402e-02,\n",
              "        -6.3429e-01,  1.5137e-02,  5.2512e-02, -4.5114e-02,  3.0619e-01,\n",
              "         2.2795e-01, -5.2012e-01,  3.1645e-01, -5.2239e-01, -4.7672e-01,\n",
              "        -3.1689e-01, -8.3288e-01,  6.8586e-01,  4.4074e-01, -6.6474e-03,\n",
              "        -6.2989e-02, -3.1122e-01,  9.6193e-02, -1.5480e-01, -5.6117e-01,\n",
              "         2.2551e-01,  7.8654e-03, -1.3355e+00, -4.3225e-01, -2.8891e-02,\n",
              "         8.5535e-01, -4.5191e-01,  8.7009e-02, -3.9219e-01, -8.1970e-01,\n",
              "        -1.0060e+00,  1.0280e-01,  1.1263e-01,  5.6084e-01,  3.0841e-01,\n",
              "        -7.0994e-01,  5.4070e-01, -4.6892e-01,  1.3429e-01,  4.6499e-01,\n",
              "         4.0507e-01, -3.7585e-01,  1.9977e-01,  4.5688e-01, -8.6294e-01,\n",
              "         5.8678e-01, -8.3593e-01, -3.5485e-01, -1.3123e-01, -9.3271e-02,\n",
              "        -6.0121e-01,  3.4541e-02, -2.0031e-01, -3.4547e-01,  7.8031e-02,\n",
              "        -3.7998e-01, -2.7716e-01,  2.7509e-01, -1.3165e-01, -8.6936e-01,\n",
              "        -5.3062e-01,  9.1694e-01,  5.3101e-01, -5.1750e-01, -2.0037e-01,\n",
              "         1.8668e-01,  7.8894e-01, -4.8112e-01, -2.5052e-01, -1.6049e-01,\n",
              "         2.1206e-02,  1.3094e-01,  6.2771e-01,  1.4436e+00, -6.0483e-01,\n",
              "        -4.2050e-01,  1.0655e+00,  2.0535e-01, -7.4213e-01,  1.3640e-01,\n",
              "         6.6067e-01,  1.0165e-01, -4.5285e-01,  6.4251e-02,  3.7258e-01,\n",
              "         8.0925e-02, -4.2181e-01, -1.6040e-02,  4.5894e-01,  1.7085e+00,\n",
              "         2.7713e-01,  3.5907e-01, -1.1778e+00,  7.8948e-01,  1.9499e+00,\n",
              "         1.1870e-01, -9.1571e-02,  6.0833e-01, -2.5049e-01,  2.6707e-01,\n",
              "         7.7882e-01, -2.6812e-01,  2.1848e-01,  1.8696e-01, -5.0655e-01,\n",
              "        -6.7893e-01,  3.0269e-01, -3.4611e-01,  7.8308e-01, -7.8303e-02,\n",
              "         4.6948e-01,  9.7181e-01,  3.4547e-01,  6.4600e-02, -4.2071e-01,\n",
              "        -1.4155e-01, -6.1170e-01, -6.2028e-01, -2.6269e-01,  4.2517e-01,\n",
              "         6.0998e-01, -1.7665e-01,  8.0595e-01,  9.2913e-01, -6.5114e-01,\n",
              "        -1.8586e-01,  4.4393e-01, -3.0261e-02,  2.2025e-01,  2.5139e-01,\n",
              "        -7.2125e-01,  4.5477e-01, -7.8619e-02,  6.0541e-02, -4.7798e-01,\n",
              "         1.7944e-01,  3.8174e-01, -6.2725e-01,  4.0156e-01,  9.3814e-01,\n",
              "         8.0615e-01, -2.1190e-01,  5.2354e-01, -9.4927e-01,  5.3650e-02,\n",
              "         2.1342e-01,  5.5072e-01, -2.1123e-01, -1.0704e-01, -3.5964e-01,\n",
              "         2.5551e-01, -9.7830e-01, -1.3628e-02,  2.7758e-01,  6.4228e-02,\n",
              "        -3.4921e-01,  8.5480e-02,  1.0661e+00,  4.9168e-01, -1.2711e-01,\n",
              "        -4.8503e-01, -6.1370e-01, -2.6764e-01,  6.2519e-01, -8.0938e-01,\n",
              "        -6.8642e-01, -1.0166e+00,  5.8970e-01,  5.1469e-01, -2.9361e-01,\n",
              "         1.1722e+00,  2.1551e-01,  1.2841e-02, -3.1239e-01, -6.8312e-01,\n",
              "        -4.2122e-01, -3.6370e-01, -3.9354e-01,  6.1687e-01,  1.5843e-01,\n",
              "        -3.6008e-01, -3.4937e-01, -1.4565e-01, -4.7284e-01, -3.0587e-01,\n",
              "        -2.2453e-01, -7.0601e-01, -3.1482e-01, -1.3291e-01,  3.3279e-01,\n",
              "         2.1077e-01, -3.8574e-01,  6.6761e-01, -7.0757e-02,  3.6814e-01,\n",
              "         1.0084e+00,  7.1724e-01,  4.6476e-01,  1.1441e-01,  2.3915e-01,\n",
              "        -7.0781e-01,  9.8382e-01,  2.4951e-01, -4.0632e-01, -5.7734e-01,\n",
              "        -1.1093e+00,  2.4006e-01, -3.3359e-02,  7.5460e-01,  5.6975e-01,\n",
              "        -2.7325e-01,  8.5772e-01, -8.1021e-01, -1.2842e+00,  2.7532e-01,\n",
              "        -4.6800e-01, -3.1463e-01, -7.5989e-01,  2.1228e-01,  1.5577e-01,\n",
              "        -1.8038e-01,  6.5861e-02, -9.8830e-02, -8.7824e-01,  2.6861e-01,\n",
              "        -4.2694e-02, -1.9849e-01, -1.1529e-01, -7.3803e-01,  1.6538e-01,\n",
              "         2.1435e-01,  3.4138e-02,  9.3150e-01,  1.0239e+00, -2.5834e-01,\n",
              "        -4.1144e-01, -2.0607e-01, -1.0059e+00,  2.8344e-01,  4.4862e-01,\n",
              "        -1.4835e-01,  8.1002e-01,  2.0263e-01,  1.9639e-01, -3.7839e-01,\n",
              "        -7.7512e-02, -3.2027e-02, -4.1397e-01,  9.5302e-02,  1.9516e-01,\n",
              "        -3.1872e-01, -2.5537e-01, -9.4945e-01, -1.5726e-01,  2.5869e-01,\n",
              "        -4.5705e-01, -8.9759e-01, -8.9033e-02], device='mps:0')"
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "\n",
        "# ---- Contextual token representations with BERT ----\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "mdl = AutoModel.from_pretrained(\"bert-base-uncased\").to(DEVICE).eval()\n",
        "mdl.eval()  # set to evaluation mode\n",
        "\n",
        "# TODO (6 pts): Implement this function to return the contextual vector for the first \"bank\" token.\n",
        "def contextual_token_vec(sentence, word=\"bank\"):\n",
        "    \"\"\"\n",
        "    Return the contextual embedding vector (torch.Tensor) for the first occurrence of `word`\n",
        "    in `sentence`, using BERT's last_hidden_state token at that position.\n",
        "    \"\"\"\n",
        "\n",
        "    # Tokenize the input sentence\n",
        "    # return_tensors=\"pt\" tells it to give us back PyTorch tensors\n",
        "    inputs = tok(sentence, return_tensors=\"pt\").to(DEVICE)\n",
        "    print(f\"tokenized: {inputs.input_ids}\")\n",
        "    \n",
        "    # not training, so no need to calculate gradients\n",
        "    with torch.no_grad():\n",
        "        outputs = mdl(**inputs)\n",
        "    \n",
        "    # Get the last hidden states (contextual embeddings)\n",
        "    # BERT are designed to process multiple sentences at once in a \"batch.\"\n",
        "    # one sentence => batch of size 1 => index is [0].\n",
        "    last_hidden_states = outputs.last_hidden_state[0]\n",
        "    \n",
        "    # Convert token IDs to tokens\n",
        "    # [0] is same reason as above, batch processing\n",
        "    tokens = tok.convert_ids_to_tokens(inputs.input_ids[0])\n",
        "    print(f\"tokens: {tokens}\")\n",
        "    \n",
        "    # BERT will split words into subwords with ## prefix if not in its vocabulary\n",
        "    word_tokens = tok.tokenize(word)\n",
        "    if len(word_tokens) != 1:\n",
        "        raise BaseException('Temporary not handle word that not in BERT vocabulary')\n",
        "\n",
        "    # Find the position of the first occurrence\n",
        "    target_position = tokens.index(word_tokens[0])\n",
        "    print(f\"target_position: {target_position}\")\n",
        "\n",
        "    # Return the contextual embedding for the target word\n",
        "    return last_hidden_states[target_position]\n",
        "\n",
        "# sentences[0] = \"Willows lined the bank of the stream.\"\n",
        "contextual_token_vec(sentences[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "WjUyOl45xKpb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tokenized: tensor([[  101, 11940,  2015,  7732,  1996,  2924,  1997,  1996,  5460,  1012,\n",
            "           102]], device='mps:0')\n",
            "tokens: ['[CLS]', 'willow', '##s', 'lined', 'the', 'bank', 'of', 'the', 'stream', '.', '[SEP]']\n",
            "target_position: 5\n",
            "tokenized: tensor([[ 101, 1045, 2441, 1037, 2047, 2924, 4070, 7483, 1012,  102]],\n",
            "       device='mps:0')\n",
            "tokens: ['[CLS]', 'i', 'opened', 'a', 'new', 'bank', 'account', 'yesterday', '.', '[SEP]']\n",
            "target_position: 5\n",
            "tokenized: tensor([[  101,  1996, 19949,  2938,  5168,  2011,  1996,  2924,  1012,   102]],\n",
            "       device='mps:0')\n",
            "tokens: ['[CLS]', 'the', 'fisherman', 'sat', 'quietly', 'by', 'the', 'bank', '.', '[SEP]']\n",
            "target_position: 7\n",
            "tokenized: tensor([[ 101, 1996, 2430, 2924, 2992, 3037, 6165, 1012,  102]],\n",
            "       device='mps:0')\n",
            "tokens: ['[CLS]', 'the', 'central', 'bank', 'raised', 'interest', 'rates', '.', '[SEP]']\n",
            "target_position: 3\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "array([[1.        , 0.37284273, 0.70163518, 0.41021445],\n",
              "       [0.37284273, 0.99999982, 0.45429575, 0.61562085],\n",
              "       [0.70163518, 0.45429575, 1.        , 0.47047782],\n",
              "       [0.41021445, 0.61562085, 0.47047782, 1.        ]])"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Use your function to get contextual vectors and build a pairwise cosine similarity matrix\n",
        "ctx_vecs = [contextual_token_vec(s, target_word) for s in sentences]\n",
        "\n",
        "pairwise_contextual = np.zeros((len(sentences), len(sentences)))\n",
        "for i, j in itertools.product(range(len(sentences)), range(len(sentences))):\n",
        "    pairwise_contextual[i, j] = float(cosine(ctx_vecs[i], ctx_vecs[j]))\n",
        "pairwise_contextual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUb02PjE3bS8"
      },
      "source": [
        "## Discussion (3 pt)\n",
        "\n",
        "**Question:** Describe what you observe in pairwise_static vs. pairwise_contextual. Why do contextual embeddings separate the riverbank and financial senses while the static vector does not?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "Word2Vec represents any word as a single vector, therefore, it does not concern about word context.\n",
        "- Mathemathic: \"bank\" words in all sentences is represent by a same vector, then, the similarity of `cosine` should be 1 (`0.99999994` may be floating point issue).\n",
        "- Meaning of the word \"bank\" is the same for the model, althought human can understand there are two concepts.\n",
        "\n",
        "BERT implements transformer\n",
        "- Each `bank` word will have different vector due to self-attention.\n",
        "- The `(river) bank` in sentence 1 and 3 and `(finance) bank` in sentences 2 and 4 are quite close toghether with similarities are `0.70163518` and `0.61562085` respectively. The model also can separate those meaning quite good with low value of similarity, aka, high difference, with `0.37284273`, `0.41021445`, `0.45429575` and `0.47047782`.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIkMTohi4HIt"
      },
      "source": [
        "# Part B — Sentence Embeddings on a Tiny STS-B Slice (9 pts)\n",
        "\n",
        "**Objective.** Evaluate sentence similarity using BERT [CLS], BERT mean pooling, and a small SBERT model by correlating cosine similarity with human similarity labels from GLUE STS-B.\n",
        "\n",
        "\t•\tDataset: GLUE → STS-B (Hugging Face)\n",
        "  \n",
        "Link: https://huggingface.co/datasets/glue\n",
        "\n",
        "Fields (validation split):\n",
        "\n",
        "• sentence1 (str)\n",
        "\n",
        "• sentence2 (str)\n",
        "\n",
        "• label (float, range 0–5; higher means more similar)\n",
        "\n",
        "•\tMetric: **Spearman rank correlation (ρ)** measures the monotonic relationship between two variables based on their ranks. **It checks whether pairs with higher human similarity also receive higher cosine similarity, without assuming linearity.**\n",
        "\n",
        "**Code Implementation (6 pts):** Implement bert_embed(sentences, strategy=\"mean\") supporting \"cls\" and \"mean\".\n",
        "\n",
        "**Discussion (3 pt):** Which strategy performs best and why might SBERT help on STS?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "UN_DpEJnxWnD"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
            "To disable this warning, you can either:\n",
            "\t- Avoid using `tokenizers` before the fork if possible\n",
            "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
          ]
        }
      ],
      "source": [
        "from datasets import load_dataset\n",
        "from scipy.stats import spearmanr, pearsonr\n",
        "\n",
        "# A tiny slice for speed\n",
        "sts = load_dataset(\"glue\", \"stsb\", split=\"validation[:200]\")\n",
        "\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "sbert_small = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "EjcNOxlExkKB"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs['attention_mask'].shape: torch.Size([2, 11])\n",
            "mask.shape: torch.Size([2, 11, 1])\n",
            "masked_embeddings.shape: torch.Size([2, 11, 768])\n",
            "summed_embeddings.shape: torch.Size([2, 768])\n",
            "token_counts.shape: torch.Size([2, 1])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "tensor([[-0.1373, -0.1593,  0.0821,  ..., -0.0644, -0.0986, -0.0170],\n",
              "        [ 0.3789,  0.0296, -0.1317,  ...,  0.1847, -0.0126,  0.1121]])"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# TODO (6 pts): Implement BERT sentence embeddings with \"cls\" and \"mean\" strategies.\n",
        "# Reuse tok/mdl from Part A\n",
        "def bert_embed(sentences, strategy=\"mean\"):\n",
        "    \"\"\"\n",
        "    Return a tensor of shape [B, H] for a list[str] `sentences`.\n",
        "    - \"cls\": use the [CLS] token embedding at position 0\n",
        "    - \"mean\": mask-aware average over token embeddings\n",
        "    \"\"\"\n",
        "    # Note, for easier reference\n",
        "    # B: batch size (num of sentences)\n",
        "    # S: max sequence length in batch\n",
        "    # H: hidden size (768 for bert-base)\n",
        "    \n",
        "    # Tokenize all sentences at once\n",
        "    # padding=True:\n",
        "    #   - Sentences in a batch have different lengths => with a special [PAD] token => rectangular tensor\n",
        "    #   - True ('longest') create shorter (20) sequences than 'max_length' (512) => higher performance\n",
        "    # return_tensors=\"pt\" tells it to give us back PyTorch tensors\n",
        "    # truncation=True: truncate a longer than max input sentence\n",
        "    # The tokenizer returns a dictionary containing 'input_ids' and 'attention_mask'\n",
        "    inputs = tok(sentences, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "    # 2. Get the BERT outputs (the token embeddings)\n",
        "    with torch.no_grad():\n",
        "        # shape is [B, S, H],\n",
        "        outputs = mdl(**inputs)\n",
        "        last_hidden_state = outputs.last_hidden_state\n",
        "\n",
        "    # 3. Apply the chosen pooling strategy\n",
        "    if strategy == \"cls\":\n",
        "        # Use the [CLS] token embedding\n",
        "        # The [CLS] token is always at the first position => index 0\n",
        "        # extracts the [CLS] embedding for every sentence in the batch => Shape: [B, H]\n",
        "        sentence_embedding = last_hidden_state[:, 0, :]\n",
        "    elif strategy == \"mean\":\n",
        "        # # ======= Ignore the padding tokens =======\n",
        "        # The attention_mask is [B, S] with 1s for real tokens and 0s for padding.\n",
        "        print(f\"inputs['attention_mask'].shape: {inputs['attention_mask'].shape}\")\n",
        "        \n",
        "        # Expand mask to match the shape of embeddings for multiplication.\n",
        "        # Shape: [B, S] => [B, S, 1]\n",
        "        mask = inputs['attention_mask'].unsqueeze(-1)\n",
        "        print(f\"mask.shape: {mask.shape}\")\n",
        "\n",
        "        # Zero out the embeddings of padding tokens.\n",
        "        # Multiplying [B, S, H] with [B, S, 1]\n",
        "        masked_embeddings = last_hidden_state * mask\n",
        "        print(f\"masked_embeddings.shape: {masked_embeddings.shape}\")\n",
        "\n",
        "\n",
        "        # # ======= Compute the mean of the embeddings =======\n",
        "        # Sum the embeddings across the sequence length dimension.\n",
        "        # Shape: [B, H]\n",
        "        summed_embeddings = torch.sum(masked_embeddings, dim=1)\n",
        "        print(f\"summed_embeddings.shape: {summed_embeddings.shape}\")\n",
        "\n",
        "        # Count the number of actual tokens in each sentence.\n",
        "        # Shape: [B]\n",
        "        token_counts = torch.sum(inputs['attention_mask'], dim=1)\n",
        "        # => Shape: [B, 1]\n",
        "        token_counts = token_counts.unsqueeze(-1)\n",
        "        print(f\"token_counts.shape: {token_counts.shape}\")\n",
        "\n",
        "        # avg = sum / count\n",
        "        # Shape: [B, H]\n",
        "        sentence_embedding = summed_embeddings / token_counts\n",
        "\n",
        "    return sentence_embedding.cpu()\n",
        "\n",
        "test_sentences = [\"Hello world!\", \"BERT is great for embeddings.\"]\n",
        "# display(bert_embed(test_sentences, strategy=\"cls\"))\n",
        "display(bert_embed(test_sentences, strategy=\"mean\"))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "psA3cJkax3FN"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "inputs['attention_mask'].shape: torch.Size([200, 20])\n",
            "mask.shape: torch.Size([200, 20, 1])\n",
            "masked_embeddings.shape: torch.Size([200, 20, 768])\n",
            "summed_embeddings.shape: torch.Size([200, 768])\n",
            "token_counts.shape: torch.Size([200, 1])\n",
            "inputs['attention_mask'].shape: torch.Size([200, 18])\n",
            "mask.shape: torch.Size([200, 18, 1])\n",
            "masked_embeddings.shape: torch.Size([200, 18, 768])\n",
            "summed_embeddings.shape: torch.Size([200, 768])\n",
            "token_counts.shape: torch.Size([200, 1])\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "{'BERT_CLS': {'spearman': 0.030008234839989045,\n",
              "  'pearson': -0.011850078466256634},\n",
              " 'BERT_MEAN': {'spearman': 0.3705995595647637, 'pearson': 0.3350414606548444},\n",
              " 'SBERT_all-MiniLM-L6-v2': {'spearman': 0.9362383986595486,\n",
              "  'pearson': 0.9287386970141315}}"
            ]
          },
          "execution_count": 12,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "def sbert_embed(sentences):\n",
        "    return torch.tensor(sbert_small.encode(list(sentences), convert_to_numpy=True))\n",
        "\n",
        "def evaluate(embed_fn):\n",
        "    s1 = [str(x) for x in sts[\"sentence1\"]]\n",
        "    s2 = [str(x) for x in sts[\"sentence2\"]]\n",
        "    a = embed_fn(s1); b = embed_fn(s2)\n",
        "    sims = torch.nn.functional.cosine_similarity(a, b).cpu().numpy()\n",
        "    gold = np.array(sts[\"label\"], dtype=float)  # 0..5\n",
        "    rho  = spearmanr(sims, gold).correlation\n",
        "    r, _ = pearsonr(sims, gold)\n",
        "    return {\"spearman\": float(rho), \"pearson\": float(r)}\n",
        "\n",
        "scores = {\n",
        "    \"BERT_CLS\":  evaluate(lambda S: bert_embed(S, \"cls\")),\n",
        "    \"BERT_MEAN\": evaluate(lambda S: bert_embed(S, \"mean\")),\n",
        "    \"SBERT_all-MiniLM-L6-v2\": evaluate(lambda S: sbert_embed(S)),\n",
        "}\n",
        "scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JtV-RNL5WSR"
      },
      "source": [
        "# Discussion (3 pt).\n",
        "**Question:** Compare the three results (especially Spearman ρ). Which strategy works best on this slice and why does SBERT (a sentence-embedding model) often outperform naive [CLS] pooling?\n",
        "\n",
        "**Answer:**\n",
        "\n",
        "`[CLS]` represent whole sentence for BERT to predict next sentence, not to compare the similarity of two sentences together, so get worst perfromance.\n",
        "\n",
        "`BERT_MEAN` takes all words of sentences together, therefore, sentences which have near word vectors in the vector space can end up as \"similar\".\n",
        "- And sentences are similar have change to contain near words in space. Then, the performance is much better than `[CLS]`\n",
        "- However, near is not nessesarity as the same. They even can have oppsited meaning.\n",
        "\n",
        "`SBERT_all-MiniLM-L6-v2` model, as its description, [can be used for tasks like clustering or semantic search](https://huggingface.co/sentence-transformers/all-MiniLM-L6-v2) which is the usecase we trying to test. So it outperform the formers.\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgbgA3Iv5mCY"
      },
      "source": [
        "# Part C — Parameter-Efficient Instruction Finetuning (LoRA on FLAN-T5-small) + Manual Training Loop (12 pts)\n",
        "\n",
        "**Objective.** Perform a tiny, multi-task instruction finetuning on SST-2 (sentiment) and BoolQ (yes/no QA) with LoRA, using a manual PyTorch training loop (**no Trainer function from HuggingFace**). Then compare zero-shot vs. finetuned accuracy.\n",
        "\n",
        "Datasets and fields:\n",
        "\n",
        "\t1.\tGLUE — SST-2 (HF): https://huggingface.co/datasets/nyu-mll/glue/viewer/sst2\n",
        "\n",
        "\t2.\tBoolQ (HF): https://huggingface.co/datasets/google/boolq\n",
        "\n",
        "*Please open the link to view dataset info, including its input and label*\n",
        "\n",
        "**Code Implementation (9 pts):** Implement the manual training loop (forward → loss → backward → clip → step → sched.step). **Using Trainer function from Huggingface is not allowed.**\n",
        "\n",
        "**Discussion (3 pt):** After comparing zero-shot vs finetuned, discuss the observed differences and why they occur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "7807Pf9OBhcE"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "(600, 400)"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
        "\n",
        "raw_sst   = load_dataset(\"glue\", \"sst2\")\n",
        "raw_boolq = load_dataset(\"boolq\")\n",
        "\n",
        "def to_instruct_sst(split, n=600):\n",
        "    ds = raw_sst[split].shuffle(seed=SEED).select(range(n if split==\"train\" else min(200, len(raw_sst[split]))))\n",
        "    def map_ex(x):\n",
        "        return {\n",
        "            \"instruction\": \"Classify the sentiment as Positive or Negative.\",\n",
        "            \"input\": x[\"sentence\"],\n",
        "            \"output\": \"Positive\" if x[\"label\"]==1 else \"Negative\",\n",
        "            \"task\": \"sst2\"\n",
        "        }\n",
        "    return ds.map(map_ex, remove_columns=ds.column_names)\n",
        "\n",
        "def to_instruct_boolq(split, n=600):\n",
        "    ds = raw_boolq[split].shuffle(seed=SEED).select(range(n if split==\"train\" else min(200, len(raw_boolq[split]))))\n",
        "    def map_ex(x):\n",
        "        return {\n",
        "            \"instruction\": \"Answer the yes/no question based on the passage.\",\n",
        "            \"input\": f\"Passage: {x['passage']}\\nQuestion: {x['question']}\",\n",
        "            \"output\": \"yes\" if x[\"answer\"] else \"no\",\n",
        "            \"task\": \"boolq\"\n",
        "        }\n",
        "    return ds.map(map_ex, remove_columns=ds.column_names)\n",
        "\n",
        "train = DatasetDict({\n",
        "    \"train\": concatenate_datasets([\n",
        "        to_instruct_sst(\"train\", n=300),\n",
        "        to_instruct_boolq(\"train\", n=300),\n",
        "    ])\n",
        "})\n",
        "\n",
        "eval_ds = DatasetDict({\n",
        "    \"validation\": concatenate_datasets([\n",
        "        to_instruct_sst(\"validation\", n=200),\n",
        "        to_instruct_boolq(\"validation\", n=200),\n",
        "    ])\n",
        "})\n",
        "\n",
        "len(train[\"train\"]), len(eval_ds[\"validation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'instruction': 'Classify the sentiment as Positive or Negative.',\n",
              " 'input': 'klein , charming in comedies like american pie and dead-on in election , ',\n",
              " 'output': 'Positive',\n",
              " 'task': 'sst2'}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/plain": [
              "{'instruction': 'Answer the yes/no question based on the passage.',\n",
              " 'input': \"Passage: Henry Daniel Mills is a fictional character in ABC's television series Once Upon a Time. Henry is the boy Emma Swan gave up to adoption; Regina Mills adopted him. Henry was originally portrayed as a child by Jared S. Gilmore, who won the Young Artist Award for Best Performance in a TV Series -- Leading Young Actor in 2012. For the show's seventh and final season, Andrew J. West later took over the role of Henry as an adult and father to a eight-year-old girl named Lucy, with Gilmore also making three appearances as Henry during the season.\\nQuestion: did henry die in once upon a time\",\n",
              " 'output': 'no',\n",
              " 'task': 'boolq'}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(train[\"train\"][0])\n",
        "display(train[\"train\"][300])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "eU4-bmrHBkn8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Map: 100%|██████████| 400/400 [00:00<00:00, 3159.76 examples/s]\n"
          ]
        }
      ],
      "source": [
        "# Tokenization for FLAN-T5 (text-to-text)\n",
        "from transformers import AutoTokenizer\n",
        "t5_name = \"google/flan-t5-small\"\n",
        "t5_tok  = AutoTokenizer.from_pretrained(t5_name)\n",
        "\n",
        "def format_example(ex):\n",
        "    prompt = f\"Instruction: {ex['instruction']}\\nInput: {ex['input']}\\nOutput:\"\n",
        "    out = t5_tok(prompt, max_length=512, truncation=True)\n",
        "    label_ids = t5_tok(ex[\"output\"], max_length=8, truncation=True).input_ids\n",
        "    out[\"labels\"] = label_ids  # list[int]\n",
        "    return out\n",
        "\n",
        "train_tokenized = train[\"train\"].map(format_example, remove_columns=train[\"train\"].column_names)\n",
        "eval_tokenized  = eval_ds[\"validation\"].map(format_example, remove_columns=eval_ds[\"validation\"].column_names)\n",
        "\n",
        "train_tokenized.set_format(type=\"torch\")\n",
        "eval_tokenized.set_format(type=\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'input_ids': tensor([21035,    10,  4501,  4921,     8,  6493,    38, 24972,    42, 17141,\n",
              "          1528,     5,    86,  2562,    10, 21856,     3,     6, 12216,    16,\n",
              "           369,  7719,   114, 10211,  6253,    11,  3654,    18,   106,    16,\n",
              "          4356,     3,     6,  3387,  2562,    10,     1]),\n",
              " 'attention_mask': tensor([1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
              "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1]),\n",
              " 'labels': tensor([24972,     1])}"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "display(train_tokenized[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "id": "1A0A5n9qIxzj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'NoneType' object has no attribute 'cadam32bit_grad_fp32'\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/opt/homebrew/Caskroom/miniconda/base/envs/nlp/lib/python3.10/site-packages/bitsandbytes/cextension.py:34: UserWarning: The installed version of bitsandbytes was compiled without GPU support. 8-bit optimizers, 8-bit multiplication, and GPU quantization are unavailable.\n",
            "  warn(\"The installed version of bitsandbytes was compiled without GPU support. \"\n"
          ]
        }
      ],
      "source": [
        "# Model + LoRA\n",
        "import math, time, torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoModelForSeq2SeqLM, get_linear_schedule_with_warmup\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(t5_name).to(DEVICE)\n",
        "peft_cfg = LoraConfig(task_type=\"SEQ_2_SEQ_LM\", r=8, lora_alpha=16, lora_dropout=0.1, target_modules=[\"q\", \"v\"])\n",
        "model = get_peft_model(base, peft_cfg).to(DEVICE)\n",
        "\n",
        "# Robust collate: pad inputs; pad labels with pad_token_id then mask to -100 for loss\n",
        "def collate_batch(features):\n",
        "    input_ids = [torch.tensor(f[\"input_ids\"], dtype=torch.long) for f in features]\n",
        "    attention_mask = [torch.tensor(f[\"attention_mask\"], dtype=torch.long) for f in features]\n",
        "    labels = [torch.tensor(f[\"labels\"], dtype=torch.long) for f in features]\n",
        "\n",
        "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=t5_tok.pad_token_id)\n",
        "    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
        "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=t5_tok.pad_token_id)\n",
        "    labels[labels == t5_tok.pad_token_id] = -100\n",
        "\n",
        "    return {\"input_ids\": input_ids.to(DEVICE), \"attention_mask\": attention_mask.to(DEVICE), \"labels\": labels.to(DEVICE)}\n",
        "\n",
        "train_loader = DataLoader(train_tokenized, batch_size=8, shuffle=True,  collate_fn=collate_batch)\n",
        "eval_loader  = DataLoader(eval_tokenized,  batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "# Pre-defined hyperparameters and scheduler\n",
        "lr = 2e-4\n",
        "epochs = 1\n",
        "max_grad_norm = 1.0\n",
        "warmup_ratio = 0.06\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "num_training_steps = epochs * math.ceil(len(train_loader))\n",
        "num_warmup_steps   = int(warmup_ratio * num_training_steps)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps,\n",
        "                                            num_training_steps=num_training_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goBXxwRj7X1j"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/var/folders/yh/9qq7z2f14f14bdj_1k2673700000gn/T/ipykernel_14148/2840696798.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  input_ids = [torch.tensor(f[\"input_ids\"], dtype=torch.long) for f in features]\n",
            "/var/folders/yh/9qq7z2f14f14bdj_1k2673700000gn/T/ipykernel_14148/2840696798.py:15: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  attention_mask = [torch.tensor(f[\"attention_mask\"], dtype=torch.long) for f in features]\n",
            "/var/folders/yh/9qq7z2f14f14bdj_1k2673700000gn/T/ipykernel_14148/2840696798.py:16: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  labels = [torch.tensor(f[\"labels\"], dtype=torch.long) for f in features]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "step    50/75 | loss 0.4961\n",
            "✅ finished in 5.2 min\n",
            "Training finished.\n"
          ]
        }
      ],
      "source": [
        "# TODO (9 pts): Implement the **manual training loop**.\n",
        "# Requirements:\n",
        "#   - For each batch: forward -> get loss -> backward -> gradient clipping -> optimizer.step() -> optimizer.zero_grad() -> scheduler.step()\n",
        "#   - Log average loss every ~50 steps (optional but recommended)\n",
        "#   - (No AMP/scaler code; use FP32)\n",
        "\n",
        "\n",
        "log_every = 50\n",
        "\n",
        "# ✨ Manual training loop —————————————————————————————————\n",
        "model.train(); t0 = time.time()\n",
        "for step, batch in enumerate(train_loader, 1):\n",
        "    batch = {k: v.to(DEVICE) for k, v in batch.items()}\n",
        "    loss  = model(**batch).loss\n",
        "    loss.backward(); optimizer.step(); scheduler.step(); optimizer.zero_grad()\n",
        "\n",
        "    if step % 50 == 0:\n",
        "        print(f\"step {step:>5}/{num_training_steps} | loss {loss.item():.4f}\")\n",
        "\n",
        "print(f\"✅ finished in {(time.time()-t0)/60:.1f} min\")\n",
        "model.save_pretrained(\"flanT5_agnews_lora\")\n",
        "\n",
        "print(\"Training finished.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "rOv4I2__J4MO"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "{'SST2': {'base': 0.83, 'finetuned': 0.835},\n",
              " 'BoolQ': {'base': 0.645, 'finetuned': 0.685}}"
            ]
          },
          "execution_count": 24,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import re\n",
        "import sys, subprocess\n",
        "def ensure(pkg):\n",
        "    try:\n",
        "        __import__(pkg)\n",
        "    except ModuleNotFoundError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "\n",
        "ensure(\"evaluate\")\n",
        "import evaluate\n",
        "acc = evaluate.load(\"accuracy\")\n",
        "\n",
        "base_eval = AutoModelForSeq2SeqLM.from_pretrained(t5_name).to(DEVICE).eval()\n",
        "ft_eval   = model.eval()\n",
        "\n",
        "def prompts_and_refs(task_name):\n",
        "    subset = [ex for ex in eval_ds[\"validation\"] if ex[\"task\"]==task_name]\n",
        "    prompts = [f\"Instruction: {ex['instruction']}\\nInput: {ex['input']}\\nOutput:\" for ex in subset]\n",
        "    refs    = [ex[\"output\"].lower().strip() for ex in subset]  # 'positive'/'negative' or 'yes'/'no'\n",
        "    return prompts, refs\n",
        "\n",
        "def batch_generate(mdl, prompts, max_new_tokens=5):\n",
        "    toks = t5_tok(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        gen = mdl.generate(**toks, max_new_tokens=max_new_tokens)\n",
        "    outs = [t5_tok.decode(g, skip_special_tokens=True).strip().lower() for g in gen]\n",
        "    return outs\n",
        "\n",
        "def normalize_pred(task, s: str):\n",
        "    t = re.findall(r\"[a-z]+\", s.lower())\n",
        "    if task == \"sst2\":\n",
        "        if any(tok.startswith(\"pos\") for tok in t): return \"positive\"\n",
        "        if any(tok.startswith(\"neg\") for tok in t): return \"negative\"\n",
        "        return \"positive\"  # fallback\n",
        "    if task == \"boolq\":\n",
        "        if \"yes\" in t or \"true\" in t:  return \"yes\"\n",
        "        if \"no\"  in t or \"false\" in t: return \"no\"\n",
        "        return \"yes\"  # fallback\n",
        "    return s.strip().lower()\n",
        "\n",
        "LABEL_ID = {\n",
        "    \"sst2\": {\"negative\": 0, \"positive\": 1},\n",
        "    \"boolq\": {\"no\": 0, \"yes\": 1},\n",
        "}\n",
        "\n",
        "def to_ids(task, items):\n",
        "    m = LABEL_ID[task]\n",
        "    return [m[x] for x in items]\n",
        "\n",
        "def eval_task(task):\n",
        "    prompts, refs = prompts_and_refs(task)\n",
        "    base_outs = [normalize_pred(task, o) for o in batch_generate(base_eval, prompts)]\n",
        "    ft_outs   = [normalize_pred(task, o) for o in batch_generate(ft_eval, prompts)]\n",
        "\n",
        "    ref_ids  = to_ids(task, refs)\n",
        "    base_ids = to_ids(task, base_outs)\n",
        "    ft_ids   = to_ids(task, ft_outs)\n",
        "\n",
        "    return {\n",
        "        \"base\": acc.compute(predictions=base_ids, references=ref_ids)[\"accuracy\"],\n",
        "        \"finetuned\": acc.compute(predictions=ft_ids, references=ref_ids)[\"accuracy\"],\n",
        "    }\n",
        "\n",
        "sst2_scores  = eval_task(\"sst2\")\n",
        "boolq_scores = eval_task(\"boolq\")\n",
        "{\"SST2\": sst2_scores, \"BoolQ\": boolq_scores}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Performance notes:\n",
        "\n",
        "epochs = 1\n",
        "```\n",
        "{'SST2': {'base': 0.83, 'finetuned': 0.835},\n",
        " 'BoolQ': {'base': 0.645, 'finetuned': 0.685}}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS9fH3p0737X"
      },
      "source": [
        "**Note:** If your fine-tuned model perform worse than the base model, you should check your code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGNF66q_8LRN"
      },
      "source": [
        "# Discussion (3 pt)\n",
        "\n",
        "**Quesion:** How did finetuning change performance compared to zero-shot for each task? Connect your observations to instruction finetuning and task specialization: why might SST-2 improve more than BoolQ (or vice versa) with this small LoRA update?\n",
        "\n",
        "*LoRA:* a parameter-efficienct tuning method which requires only tuning a small amount of parameters instead of full-parameter updates\n",
        "\n",
        "*Hint:* The FLAN-T5 model was already a fine-tuned version of the base T5 model. You can find online information/papers regarding with which kinds of tasks FLAN-T5 was already fine-tuned.\n",
        "\n",
        "**Answer:**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
