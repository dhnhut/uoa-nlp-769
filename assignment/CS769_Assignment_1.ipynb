{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jVx0taBG01Fd"
      },
      "source": [
        "# COMPSCI 769 - Assignment 1\n",
        "\n",
        "**Total Points:** 30  \n",
        "**Runtime:** Colab **T4 GPU**  \n",
        "**Dependencies:** `transformers`, `datasets`, `peft`, `accelerate`, `sentence-transformers`, `evaluate`, `scipy`, `gensim`\n",
        "**Objectives:**\n",
        "\n",
        "\n",
        "1.   Assessing your understanding on the difference between static word embeddings and contextual word embeddings\n",
        "2.   Assessing your understanding on different token feature fusion strategies of BERT\n",
        "3. Assessing your ability of conducting instruction fine-tuning on pre-trained generative language models and understanding on its corresponding effects.\n",
        "\n",
        "\n",
        "\n",
        "---\n",
        "**Note:** There is no need to install dependencies other than the gensim. Google Colab has already setup this environment for you. Enforcing downloading dependencies of newer versions may airse package conflicts. Just follow the `pip install` commands already provided in this assignment.\n",
        "\n",
        "**Please keep the console outputs, and do not clear them for marking easiness**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z_fDFEIPB2UY"
      },
      "source": [
        "**Name:**\n",
        "\n",
        "**UPI:**\n",
        "\n",
        "**Student ID:**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uadbUKksw-YR"
      },
      "outputs": [],
      "source": [
        "# ✅ Setup\n",
        "import os, random, numpy as np, torch\n",
        "SEED = 42\n",
        "random.seed(SEED); np.random.seed(SEED); torch.manual_seed(SEED)\n",
        "if torch.cuda.is_available():\n",
        "    torch.cuda.manual_seed_all(SEED)\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "DEVICE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S3BCO-X82XeB"
      },
      "source": [
        "# Part A — Static (Word2Vec) vs. Contextual (BERT) Token Representations (9 pts)\n",
        "\n",
        "**Objective.** Show that **static** word vectors (one vector per token type) cannot disambiguate polysemy (e.g., bank), while **contextual** token embeddings group occurrences by sense (e.g., riverbank vs. financial).\n",
        "\n",
        "**Code Implementation (6 pts):** Implement `contextual_token_vec(sentence, word=\"bank\")`.\n",
        "\n",
        "**Discussion (3 pt):** Explain your observed similarity patterns and why they occur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "pq7bokg7xEjv"
      },
      "outputs": [],
      "source": [
        "!pip install gensim\n",
        "import torch, numpy as np, itertools\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "sentences = [\n",
        "    \"Willows lined the bank of the stream.\",\n",
        "    \"I opened a new bank account yesterday.\",\n",
        "    \"The fisherman sat quietly by the bank.\",\n",
        "    \"The central bank raised interest rates.\"\n",
        "]\n",
        "target_word = \"bank\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4_-5PgCL3IUL"
      },
      "source": [
        "**Note:** *After installing the gensim package, please restart the session following the instructions printed in the console.*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KFEfk7yI0-NS"
      },
      "outputs": [],
      "source": [
        "# ---- Static baseline: small Word2Vec trained quickly on a slice of text8 ----\n",
        "import torch, numpy as np, itertools\n",
        "import gensim.downloader as api\n",
        "from gensim.models import Word2Vec\n",
        "\n",
        "# Load a manageable slice for speed\n",
        "text8 = api.load(\"text8\")                          # generator of tokenized sentences\n",
        "sentences_w2v = [s for _, s in zip(range(50_000), text8)]  # ~50k lines\n",
        "\n",
        "# Train a compact skip-gram Word2Vec\n",
        "w2v = Word2Vec(\n",
        "    sentences=sentences_w2v,\n",
        "    vector_size=100, window=5, min_count=5, workers=2,\n",
        "    sg=1, epochs=3\n",
        ")\n",
        "\n",
        "# One static vector for the token type \"bank\"\n",
        "w2v_bank = torch.tensor(w2v.wv[target_word]).float()\n",
        "\n",
        "def cosine(a, b):\n",
        "    return torch.nn.functional.cosine_similarity(a, b, dim=-1)\n",
        "\n",
        "# Pairwise similarities under the SAME static \"bank\" vector\n",
        "reps_static = torch.stack([w2v_bank for _ in sentences])  # [N, D]\n",
        "pairwise_static = np.zeros((len(sentences), len(sentences)))\n",
        "for i, j in itertools.product(range(len(sentences)), range(len(sentences))):\n",
        "    pairwise_static[i, j] = float(cosine(reps_static[i], reps_static[j]))\n",
        "pairwise_static  # expected ~ all ones"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WjUyOl45xKpb"
      },
      "outputs": [],
      "source": [
        "# ---- Contextual token representations with BERT ----\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "\n",
        "tok = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "mdl = AutoModel.from_pretrained(\"bert-base-uncased\").to(DEVICE).eval()\n",
        "\n",
        "# TODO (6 pts): Implement this function to return the contextual vector for the first \"bank\" token.\n",
        "def contextual_token_vec(sentence, word=\"bank\"):\n",
        "    \"\"\"\n",
        "    Return the contextual embedding vector (torch.Tensor) for the first occurrence of `word`\n",
        "    in `sentence`, using BERT's last_hidden_state token at that position.\n",
        "    \"\"\"\n",
        "    # Your code here\n",
        "    raise NotImplementedError\n",
        "\n",
        "# Use your function to get contextual vectors and build a pairwise cosine similarity matrix\n",
        "ctx_vecs = [contextual_token_vec(s, target_word) for s in sentences]\n",
        "\n",
        "pairwise_contextual = np.zeros((len(sentences), len(sentences)))\n",
        "for i, j in itertools.product(range(len(sentences)), range(len(sentences))):\n",
        "    pairwise_contextual[i, j] = float(cosine(ctx_vecs[i], ctx_vecs[j]))\n",
        "pairwise_contextual"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CUb02PjE3bS8"
      },
      "source": [
        "## Discussion (3 pt)\n",
        "\n",
        "**Question:** Describe what you observe in pairwise_static vs. pairwise_contextual. Why do contextual embeddings separate the riverbank and financial senses while the static vector does not?\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qIkMTohi4HIt"
      },
      "source": [
        "# Part B — Sentence Embeddings on a Tiny STS-B Slice (9 pts)\n",
        "\n",
        "**Objective.** Evaluate sentence similarity using BERT [CLS], BERT mean pooling, and a small SBERT model by correlating cosine similarity with human similarity labels from GLUE STS-B.\n",
        "\n",
        "\t•\tDataset: GLUE → STS-B (Hugging Face)\n",
        "  \n",
        "Link: https://huggingface.co/datasets/glue\n",
        "\n",
        "Fields (validation split):\n",
        "\n",
        "• sentence1 (str)\n",
        "\n",
        "• sentence2 (str)\n",
        "\n",
        "• label (float, range 0–5; higher means more similar)\n",
        "\n",
        "•\tMetric: **Spearman rank correlation (ρ)** measures the monotonic relationship between two variables based on their ranks. **It checks whether pairs with higher human similarity also receive higher cosine similarity, without assuming linearity.**\n",
        "\n",
        "**Code Implementation (6 pts):** Implement bert_embed(sentences, strategy=\"mean\") supporting \"cls\" and \"mean\".\n",
        "\n",
        "**Discussion (3 pt):** Which strategy performs best and why might SBERT help on STS?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UN_DpEJnxWnD"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "from scipy.stats import spearmanr, pearsonr\n",
        "\n",
        "# A tiny slice for speed\n",
        "sts = load_dataset(\"glue\", \"stsb\", split=\"validation[:200]\")\n",
        "\n",
        "\n",
        "from sentence_transformers import SentenceTransformer\n",
        "sbert_small = SentenceTransformer(\"all-MiniLM-L6-v2\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EjcNOxlExkKB"
      },
      "outputs": [],
      "source": [
        "# TODO (6 pts): Implement BERT sentence embeddings with \"cls\" and \"mean\" strategies.\n",
        "# Reuse tok/mdl from Part A\n",
        "def bert_embed(sentences, strategy=\"mean\"):\n",
        "    \"\"\"\n",
        "    Return a tensor of shape [B, H] for a list[str] `sentences`.\n",
        "    - \"cls\": use the [CLS] token embedding at position 0\n",
        "    - \"mean\": mask-aware average over token embeddings\n",
        "    \"\"\"\n",
        "    # Your code here\n",
        "    raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "psA3cJkax3FN"
      },
      "outputs": [],
      "source": [
        "def sbert_embed(sentences):\n",
        "    return torch.tensor(sbert_small.encode(list(sentences), convert_to_numpy=True))\n",
        "\n",
        "def evaluate(embed_fn):\n",
        "    s1 = [str(x) for x in sts[\"sentence1\"]]\n",
        "    s2 = [str(x) for x in sts[\"sentence2\"]]\n",
        "    a = embed_fn(s1); b = embed_fn(s2)\n",
        "    sims = torch.nn.functional.cosine_similarity(a, b).cpu().numpy()\n",
        "    gold = np.array(sts[\"label\"], dtype=float)  # 0..5\n",
        "    rho  = spearmanr(sims, gold).correlation\n",
        "    r, _ = pearsonr(sims, gold)\n",
        "    return {\"spearman\": float(rho), \"pearson\": float(r)}\n",
        "\n",
        "scores = {\n",
        "    \"BERT_CLS\":  evaluate(lambda S: bert_embed(S, \"cls\")),\n",
        "    \"BERT_MEAN\": evaluate(lambda S: bert_embed(S, \"mean\")),\n",
        "    \"SBERT_all-MiniLM-L6-v2\": evaluate(lambda S: sbert_embed(S)),\n",
        "}\n",
        "scores"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_JtV-RNL5WSR"
      },
      "source": [
        "# Discussion (3 pt).\n",
        "**Question:** Compare the three results (especially Spearman ρ). Which strategy works best on this slice and why does SBERT (a sentence-embedding model) often outperform naive [CLS] pooling?\n",
        "\n",
        "**Answer:**"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XgbgA3Iv5mCY"
      },
      "source": [
        "# Part C — Parameter-Efficient Instruction Finetuning (LoRA on FLAN-T5-small) + Manual Training Loop (12 pts)\n",
        "\n",
        "**Objective.** Perform a tiny, multi-task instruction finetuning on SST-2 (sentiment) and BoolQ (yes/no QA) with LoRA, using a manual PyTorch training loop (**no Trainer function from HuggingFace**). Then compare zero-shot vs. finetuned accuracy.\n",
        "\n",
        "Datasets and fields:\n",
        "\n",
        "\t1.\tGLUE — SST-2 (HF): https://huggingface.co/datasets/nyu-mll/glue/viewer/sst2\n",
        "\n",
        "\t2.\tBoolQ (HF): https://huggingface.co/datasets/google/boolq\n",
        "\n",
        "*Please open the link to view dataset info, including its input and label*\n",
        "\n",
        "**Code Implementation (9 pts):** Implement the manual training loop (forward → loss → backward → clip → step → sched.step). **Using Trainer function from Huggingface is not allowed.**\n",
        "\n",
        "**Discussion (3 pt):** After comparing zero-shot vs finetuned, discuss the observed differences and why they occur."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7807Pf9OBhcE"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset, DatasetDict, concatenate_datasets\n",
        "\n",
        "raw_sst   = load_dataset(\"glue\", \"sst2\")\n",
        "raw_boolq = load_dataset(\"boolq\")\n",
        "\n",
        "def to_instruct_sst(split, n=600):\n",
        "    ds = raw_sst[split].shuffle(seed=SEED).select(range(n if split==\"train\" else min(200, len(raw_sst[split]))))\n",
        "    def map_ex(x):\n",
        "        return {\n",
        "            \"instruction\": \"Classify the sentiment as Positive or Negative.\",\n",
        "            \"input\": x[\"sentence\"],\n",
        "            \"output\": \"Positive\" if x[\"label\"]==1 else \"Negative\",\n",
        "            \"task\": \"sst2\"\n",
        "        }\n",
        "    return ds.map(map_ex, remove_columns=ds.column_names)\n",
        "\n",
        "def to_instruct_boolq(split, n=600):\n",
        "    ds = raw_boolq[split].shuffle(seed=SEED).select(range(n if split==\"train\" else min(200, len(raw_boolq[split]))))\n",
        "    def map_ex(x):\n",
        "        return {\n",
        "            \"instruction\": \"Answer the yes/no question based on the passage.\",\n",
        "            \"input\": f\"Passage: {x['passage']}\\nQuestion: {x['question']}\",\n",
        "            \"output\": \"yes\" if x[\"answer\"] else \"no\",\n",
        "            \"task\": \"boolq\"\n",
        "        }\n",
        "    return ds.map(map_ex, remove_columns=ds.column_names)\n",
        "\n",
        "train = DatasetDict({\n",
        "    \"train\": concatenate_datasets([\n",
        "        to_instruct_sst(\"train\", n=300),\n",
        "        to_instruct_boolq(\"train\", n=300),\n",
        "    ])\n",
        "})\n",
        "\n",
        "eval_ds = DatasetDict({\n",
        "    \"validation\": concatenate_datasets([\n",
        "        to_instruct_sst(\"validation\", n=200),\n",
        "        to_instruct_boolq(\"validation\", n=200),\n",
        "    ])\n",
        "})\n",
        "\n",
        "len(train[\"train\"]), len(eval_ds[\"validation\"])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eU4-bmrHBkn8"
      },
      "outputs": [],
      "source": [
        "# Tokenization for FLAN-T5 (text-to-text)\n",
        "from transformers import AutoTokenizer\n",
        "t5_name = \"google/flan-t5-small\"\n",
        "t5_tok  = AutoTokenizer.from_pretrained(t5_name)\n",
        "\n",
        "def format_example(ex):\n",
        "    prompt = f\"Instruction: {ex['instruction']}\\nInput: {ex['input']}\\nOutput:\"\n",
        "    out = t5_tok(prompt, max_length=512, truncation=True)\n",
        "    label_ids = t5_tok(ex[\"output\"], max_length=8, truncation=True).input_ids\n",
        "    out[\"labels\"] = label_ids  # list[int]\n",
        "    return out\n",
        "\n",
        "train_tokenized = train[\"train\"].map(format_example, remove_columns=train[\"train\"].column_names)\n",
        "eval_tokenized  = eval_ds[\"validation\"].map(format_example, remove_columns=eval_ds[\"validation\"].column_names)\n",
        "\n",
        "train_tokenized.set_format(type=\"torch\")\n",
        "eval_tokenized.set_format(type=\"torch\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1A0A5n9qIxzj"
      },
      "outputs": [],
      "source": [
        "# Model + LoRA\n",
        "import math, time, torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torch.optim import AdamW\n",
        "from transformers import AutoModelForSeq2SeqLM, get_linear_schedule_with_warmup\n",
        "from peft import LoraConfig, get_peft_model\n",
        "\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(t5_name).to(DEVICE)\n",
        "peft_cfg = LoraConfig(task_type=\"SEQ_2_SEQ_LM\", r=8, lora_alpha=16, lora_dropout=0.1, target_modules=[\"q\", \"v\"])\n",
        "model = get_peft_model(base, peft_cfg).to(DEVICE)\n",
        "\n",
        "# Robust collate: pad inputs; pad labels with pad_token_id then mask to -100 for loss\n",
        "def collate_batch(features):\n",
        "    input_ids = [torch.tensor(f[\"input_ids\"], dtype=torch.long) for f in features]\n",
        "    attention_mask = [torch.tensor(f[\"attention_mask\"], dtype=torch.long) for f in features]\n",
        "    labels = [torch.tensor(f[\"labels\"], dtype=torch.long) for f in features]\n",
        "\n",
        "    input_ids = torch.nn.utils.rnn.pad_sequence(input_ids, batch_first=True, padding_value=t5_tok.pad_token_id)\n",
        "    attention_mask = torch.nn.utils.rnn.pad_sequence(attention_mask, batch_first=True, padding_value=0)\n",
        "    labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value=t5_tok.pad_token_id)\n",
        "    labels[labels == t5_tok.pad_token_id] = -100\n",
        "\n",
        "    return {\"input_ids\": input_ids.to(DEVICE), \"attention_mask\": attention_mask.to(DEVICE), \"labels\": labels.to(DEVICE)}\n",
        "\n",
        "train_loader = DataLoader(train_tokenized, batch_size=8, shuffle=True,  collate_fn=collate_batch)\n",
        "eval_loader  = DataLoader(eval_tokenized,  batch_size=8, shuffle=False, collate_fn=collate_batch)\n",
        "\n",
        "# Pre-defined hyperparameters and scheduler\n",
        "lr = 2e-4\n",
        "epochs = 1\n",
        "max_grad_norm = 1.0\n",
        "warmup_ratio = 0.06\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=lr)\n",
        "num_training_steps = epochs * math.ceil(len(train_loader))\n",
        "num_warmup_steps   = int(warmup_ratio * num_training_steps)\n",
        "scheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=num_warmup_steps,\n",
        "                                            num_training_steps=num_training_steps)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goBXxwRj7X1j"
      },
      "outputs": [],
      "source": [
        "# TODO (9 pts): Implement the **manual training loop**.\n",
        "# Requirements:\n",
        "#   - For each batch: forward -> get loss -> backward -> gradient clipping -> optimizer.step() -> optimizer.zero_grad() -> scheduler.step()\n",
        "#   - Log average loss every ~50 steps (optional but recommended)\n",
        "#   - (No AMP/scaler code; use FP32)\n",
        "\n",
        "\n",
        "log_every = 50\n",
        "model.train()\n",
        "global_step = 0\n",
        "running = 0.0\n",
        "start_time = time.time()\n",
        "\n",
        "# Your code here\n",
        "raise NotImplementedError"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rOv4I2__J4MO"
      },
      "outputs": [],
      "source": [
        "import re\n",
        "import sys, subprocess\n",
        "def ensure(pkg):\n",
        "    try:\n",
        "        __import__(pkg)\n",
        "    except ModuleNotFoundError:\n",
        "        subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", \"-q\", pkg])\n",
        "\n",
        "ensure(\"evaluate\")\n",
        "import evaluate\n",
        "acc = evaluate.load(\"accuracy\")\n",
        "\n",
        "base_eval = AutoModelForSeq2SeqLM.from_pretrained(t5_name).to(DEVICE).eval()\n",
        "ft_eval   = model.eval()\n",
        "\n",
        "def prompts_and_refs(task_name):\n",
        "    subset = [ex for ex in eval_ds[\"validation\"] if ex[\"task\"]==task_name]\n",
        "    prompts = [f\"Instruction: {ex['instruction']}\\nInput: {ex['input']}\\nOutput:\" for ex in subset]\n",
        "    refs    = [ex[\"output\"].lower().strip() for ex in subset]  # 'positive'/'negative' or 'yes'/'no'\n",
        "    return prompts, refs\n",
        "\n",
        "def batch_generate(mdl, prompts, max_new_tokens=5):\n",
        "    toks = t5_tok(prompts, padding=True, truncation=True, return_tensors=\"pt\").to(DEVICE)\n",
        "    with torch.no_grad():\n",
        "        gen = mdl.generate(**toks, max_new_tokens=max_new_tokens)\n",
        "    outs = [t5_tok.decode(g, skip_special_tokens=True).strip().lower() for g in gen]\n",
        "    return outs\n",
        "\n",
        "def normalize_pred(task, s: str):\n",
        "    t = re.findall(r\"[a-z]+\", s.lower())\n",
        "    if task == \"sst2\":\n",
        "        if any(tok.startswith(\"pos\") for tok in t): return \"positive\"\n",
        "        if any(tok.startswith(\"neg\") for tok in t): return \"negative\"\n",
        "        return \"positive\"  # fallback\n",
        "    if task == \"boolq\":\n",
        "        if \"yes\" in t or \"true\" in t:  return \"yes\"\n",
        "        if \"no\"  in t or \"false\" in t: return \"no\"\n",
        "        return \"yes\"  # fallback\n",
        "    return s.strip().lower()\n",
        "\n",
        "LABEL_ID = {\n",
        "    \"sst2\": {\"negative\": 0, \"positive\": 1},\n",
        "    \"boolq\": {\"no\": 0, \"yes\": 1},\n",
        "}\n",
        "\n",
        "def to_ids(task, items):\n",
        "    m = LABEL_ID[task]\n",
        "    return [m[x] for x in items]\n",
        "\n",
        "def eval_task(task):\n",
        "    prompts, refs = prompts_and_refs(task)\n",
        "    base_outs = [normalize_pred(task, o) for o in batch_generate(base_eval, prompts)]\n",
        "    ft_outs   = [normalize_pred(task, o) for o in batch_generate(ft_eval, prompts)]\n",
        "\n",
        "    ref_ids  = to_ids(task, refs)\n",
        "    base_ids = to_ids(task, base_outs)\n",
        "    ft_ids   = to_ids(task, ft_outs)\n",
        "\n",
        "    return {\n",
        "        \"base\": acc.compute(predictions=base_ids, references=ref_ids)[\"accuracy\"],\n",
        "        \"finetuned\": acc.compute(predictions=ft_ids, references=ref_ids)[\"accuracy\"],\n",
        "    }\n",
        "\n",
        "sst2_scores  = eval_task(\"sst2\")\n",
        "boolq_scores = eval_task(\"boolq\")\n",
        "{\"SST2\": sst2_scores, \"BoolQ\": boolq_scores}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fS9fH3p0737X"
      },
      "source": [
        "**Note:** If your fine-tuned model perform worse than the base model, you should check your code."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NGNF66q_8LRN"
      },
      "source": [
        "# Discussion (3 pt)\n",
        "\n",
        "**Quesion:** How did finetuning change performance compared to zero-shot for each task? Connect your observations to instruction finetuning and task specialization: why might SST-2 improve more than BoolQ (or vice versa) with this small LoRA update?\n",
        "\n",
        "*LoRA:* a parameter-efficienct tuning method which requires only tuning a small amount of parameters instead of full-parameter updates\n",
        "\n",
        "*Hint:* The FLAN-T5 model was already a fine-tuned version of the base T5 model. You can find online information/papers regarding with which kinds of tasks FLAN-T5 was already fine-tuned.\n",
        "\n",
        "**Answer:**"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "nlp",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.18"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
